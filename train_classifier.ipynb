{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Train Classifier with TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import jieba\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## type dict\n",
    "Grammar = {'完成式': 1, '進行式': 2, '過去式': 3, '未來式': 4, '關係代名詞': 5, '不定詞': 6, '名詞子句': 7, \n",
    "           '被動': 8, '介係詞': 9, '連接詞': 10, '假設語氣': 11, '分詞': 12, 'PT': 13, '其它': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Grammar = {'1': '完成式', '2': '進行式', '3': '過去式', '4': '未來式', '5': '關係代名詞', '6': '不定詞', '7': '名詞子句', '8': '被動', '9': '介係詞', \\\n",
    "           '10': '連接詞', '11': '假設語氣', '12': '分詞', '13': 'PT', '0': '其它'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('questions_nondup_dup.csv') as csvfile:\n",
    "    data_dict = defaultdict()\n",
    "    for row in csv.DictReader(csvfile):\n",
    "        data_dict[row['question_id']] = row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class DataHelper(object):\n",
    "    def __init__(self, file):\n",
    "        self.file = file\n",
    "        self.stopwords = ['什麼', '請問', '這裡', '不是', '意思', '這邊', '謝謝', '這句', '為何', '使用', '怎麼', '要加', '老師', '還是', '如何', '甚麼', '一下', '這個', '這樣', '問為', '因為', '何要', '用過', '是不是', '一個', '應該', '直接', '好像', '如果', '何不', '兩個', '這是', '何用', '需要', '時候', '所以', '您好', '起來', '還有', '加上', '寫成', '你好', '此句', '有點', '問此', '不好意思', '不到', '像是', '這裏', '為什麼']\n",
    "        \n",
    "        with open('{0}'.format(self.file)) as data_file:\n",
    "            self.unamb_data = defaultdict(list)\n",
    "            self.amb_data = defaultdict(list)\n",
    "            for row in csv.DictReader(data_file):\n",
    "                if row['ambiguous'] == '0':\n",
    "                    # can't directly use row.values() as it doesn't grantee the order\n",
    "                    self.unamb_data[row['type']].append([row['question_id'], row['member_id'], \\\n",
    "                                                         row['type'], row['question'], row['ambiguous']])\n",
    "                else:\n",
    "                    self.amb_data[row['type']].append([row['question_id'], row['member_id'], \\\n",
    "                                                         row['type'], row['question'], row['ambiguous']])\n",
    "                    \n",
    "    def get_all_unambiguous_data(self):\n",
    "        X = []\n",
    "        y = []\n",
    "        member_id = []\n",
    "        question_id = []\n",
    "        for key, record in self.unamb_data.items():\n",
    "            if key == '13':\n",
    "                continue\n",
    "            questions = list(list(zip(*record))[3]) \n",
    "            members = list(list(zip(*record))[1]) # get memberid list from records\n",
    "            question_idx = list(list(zip(*record))[0])\n",
    "            X += questions\n",
    "            y += [key]*len(questions)\n",
    "            member_id += members\n",
    "            question_id += question_idx\n",
    "            \n",
    "        X_text = self.cut_questions(X)\n",
    "        return X_text, np.array(y), member_id, question_id\n",
    "        \n",
    "    def get_shuffled_data(self, ratio = 8):\n",
    "        X_train = []\n",
    "        X_test = []\n",
    "        Y_train = []\n",
    "        Y_test = []\n",
    "        member_train = []\n",
    "        member_test = []\n",
    "        question_train = []\n",
    "        question_test = []\n",
    "        for key, record in self.unamb_data.items():\n",
    "            if key == '13':\n",
    "                continue\n",
    "            questions = list(list(zip(*record))[3]) # get question list from records\n",
    "            members = list(list(zip(*record))[1]) # get memberid list from records\n",
    "            question_idx = list(list(zip(*record))[0])\n",
    "            random.shuffle(questions)\n",
    "            split_point = len(questions)*ratio//10\n",
    "            train = questions[:split_point]\n",
    "            test = questions[split_point:]\n",
    "            member_train += members[:split_point]\n",
    "            member_test += members[split_point:]\n",
    "            question_train += question_idx[:split_point]\n",
    "            question_test += question_idx[split_point:]\n",
    "            X_train += train\n",
    "            X_test += test\n",
    "            Y_train += [key]*len(train) # repeat len(train) times\n",
    "            Y_test += [key]*len(test)\n",
    "            \n",
    "        X_train_text = self.cut_questions(X_train)\n",
    "        X_test_text = self.cut_questions(X_test)\n",
    "        return X_train_text, np.array(Y_train), X_test_text, np.array(Y_test), member_train, member_test, question_train, question_test\n",
    "    \n",
    "    # use non-duplications as training and duplications as testing\n",
    "    # the file should be questions_nondup_dup.csv\n",
    "    def get_fixed_data(self):\n",
    "        X_train = []\n",
    "        X_test = []\n",
    "        Y_train = []\n",
    "        Y_test = []\n",
    "        member_train = []\n",
    "        member_test = []\n",
    "        question_train = []\n",
    "        question_test = []\n",
    "        \n",
    "        for key, record in self.unamb_data.items():\n",
    "            if key == '13':\n",
    "                continue\n",
    "            questions = list(list(zip(*record))[3]) # get question list from records\n",
    "            members = list(list(zip(*record))[1]) # get memberid list from records\n",
    "            question_idx = list(list(zip(*record))[0])\n",
    "            X_train += questions\n",
    "            Y_train += [key]*len(questions)\n",
    "            member_train += members\n",
    "            question_train += question_idx\n",
    "        for key, record in self.amb_data.items():\n",
    "            if key == '13':\n",
    "                continue\n",
    "            questions = list(list(zip(*record))[3]) # get question list from records\n",
    "            members = list(list(zip(*record))[1]) # get memberid list from records\n",
    "            question_idx = list(list(zip(*record))[0])\n",
    "            X_test += questions\n",
    "            Y_test += [key]*len(questions)\n",
    "            member_test += members\n",
    "            question_test += question_idx\n",
    "            \n",
    "        X_train_text = self.cut_questions(X_train)\n",
    "        X_test_text = self.cut_questions(X_test)\n",
    "        return X_train_text, np.array(Y_train), X_test_text, np.array(Y_test), member_train, member_test, question_train, question_test\n",
    "        \n",
    "    def cut_questions(self, data):\n",
    "        corpus = []\n",
    "        for q in data:\n",
    "            segs = jieba.cut(q, cut_all=False)\n",
    "            final = [seg for seg in segs if seg not in self.stopwords]\n",
    "            corpus.append(' '.join(final))\n",
    "        return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['30383', '56291', '2', '這裡的 letting 加ing是因為also的關係嗎?  You wanna be getting to know a person 是未來進行式吧?並不是因為 also 的關係喔，是因為接續了前面的 wanna be，完整一點應該是： ...and you also wanna be letting that person get to know you.  因為和前面共用一個 wanna be，所以後面省略。  這裡因為沒有 will，所以不算是未來進行式，但現在進行式本身也有未來的意涵喔。', '0'], ['28854', '56291', '2', \"you'll be speaking 為甚麼speak要加ing 還有這句要如何解釋這裡是「未來進行式」的用法，表示某一動作將會、或可能在未來某一時刻進行或持續進行中。  you'll be speaking 就是「你未來、以後都會這樣說」的意思。  這裡用 you'll speak 當然也沒有問題，只是語意上有些許差別而已。\", '0']]\n",
      "[['32440', '88024', '2', '為什麼wanna後面要用be getting而不是直接用wanna get to ... ??這裡是在 want to 後面加上現在進行式（be 動詞+現在分詞）的用法，有「一直做、到未來也要做這件事」的口吻。  當然也可以只用 want to get to know，只是口氣上有些微差別而已，但兩種表達方式大致意思是一樣的。', '1'], ['29663', '56291', '2', \" be making friends 這裡是未來進行式嗎?   throughout 這裡昰介係詞嗎您好！  1. 是的，這裡用 going to be making friends 表示現在、未來都會要去交朋友的意思，有動作延續的口吻。當然直接寫成 we're going to make friends 也是完全沒問題的，只是口吻上有一些些差別而已。  2. throughout 在這裡是介係詞沒錯喔，另外也可以參考字典上第 1 條解釋： https://tw.dictionary.yahoo.com/dictionary?p=throughout \", '1']]\n"
     ]
    }
   ],
   "source": [
    "dh = DataHelper('questions_nondup_dup2.csv')\n",
    "print(dh.unamb_data['2'][:2])\n",
    "print(dh.amb_data['2'][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### get shuffled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape: (3095, 9652)\n",
      "y train shape: (3095,)\n"
     ]
    }
   ],
   "source": [
    "X_train_text, y_train, X_test_text, y_test, member_train, member_test, question_train, question_test = dh.get_shuffled_data()\n",
    "print('X train shape: {}'.format(X_train.shape))\n",
    "print('y train shape: {}'.format(y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## extract features of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class TextFeature(object):\n",
    "    def __init__(self, training_data, testing_data):\n",
    "        self.training_text = training_data\n",
    "        self.testing_text = testing_data\n",
    "        \n",
    "    def get_tfidf(self, use_idf = True):\n",
    "#         texts = self.training_text + self.testing_text\n",
    "        tfidf_vectorizer = TfidfVectorizer(use_idf = True)\n",
    "        tfidf_vectorizer.fit(self.training_text)\n",
    "        X_train = tfidf_vectorizer.transform(self.training_text)\n",
    "        X_test = None\n",
    "        if self.testing_text != None:\n",
    "            X_test = tfidf_vectorizer.transform(self.testing_text)\n",
    "        return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## get features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3095, 9680)\n",
      "(780, 9680)\n"
     ]
    }
   ],
   "source": [
    "tf = TextFeature(X_train_text, X_test_text)\n",
    "X_train, X_test = tf.get_tfidf()\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Corss Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Get all data and get features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['She   told   me   after   she   was   diagnosed   that   death   was   not   what   saddened   her   the   most .     請 加 what 的   what   saddened   her   the   most   是 一整 個 名詞 子句 ， what   是 「 所 ... 的 事物 」 的 ，   what   saddened   her   the   most   就是 「 讓 她 最 傷心 的 事 」 的 。       death   was   not   what   saddened   her   the   most   就是 在 說 ： 死亡 並不事 讓 她 最 傷心 的 事 。     類似 用法 可以 參考 奇摩 字典 ， what   的 第三 個 解釋 ：   https : / / tw . dictionary . yahoo . com / dictionary ? p = what', '在   caused   前 省略 了 that   was     那平時 ， 我們 該 怎麼察覺 原本 有 東西 被 省略 呢 ?   然 後 是 名詞 子句 嗎 ? 這 就是 平時 我們 會察覺 有 省略 的 ， 小老師 很 難 告訴 你 要 觀察 ， 但是 我們 在 教材 有 省略 的 地方 都 一定 會解 說 ， 這 真的 就是 要 靠 多 去 練習 、 熟悉 這種 用法 ， 自然 就 會 看得出 來 了 。     另外 ， 一句 完整 的 句子 ， 並 名詞 子句 ， 子句 是 用 在 有 主詞 動詞 但是 「 不 完整 」 的 句子 裡 。', '為 的 句型 結構 Whether   S + V ,   S + V   呢 也 有 whether   or   not ... 的 用法 喔 。   一定 要 把 or   not 拿到 前面 來 ， 是 these   insects   go   out   experiencing   the   greatest   caffeine   high   ever 太長 了 。 其實 結構 是 可以 理解   Whether     ( these   insects   go   out   experiencing   the   greatest   caffeine   high   ever )   or   not               is   not   known .   Whether   ( S + V )   or   not         is   not   known     Whether   or   not   ( S + V )           is   not   known   = >   其中 的 Whether   ( S + V )   or   not 再 把 它 當成 S ,   配合 接下 來 的 is   not   known 算是 又 句子 結構   沒錯 ！ Whether   or   not   these   insects   go   out   experiencing   the   greatest   caffeine   high   ever   整個 是 名詞 子句 ， 雖是 有動詞 的 子句 ， 但 具名 詞 形式 ， 在 此當 作主 詞 。 不用 客氣 ， 有 問題 都 歡迎 多多 發問 喔 ！']\n",
      "[\"「 They   say   the   lion ' s   visit   gives   us   all   good   luck . 」   這段 句子 的 say 、 visit 、 gives 都 是 動詞 ? 為 句子 可以 有 以上 的 動詞 組成 嗎 ? 的 主要 動詞 是 前面 的   say ， say   後 面其實 省略 了   that ， 原本 是 ：   They   say   that   the   lion ' s   visit   gives   us   all   good   luck .       that   是 連接 詞 的 用法 ， 因此 後 面 可以 再 接上 有主詞 、 有動詞 的 完整 句子 。 那 的   visit   並 動詞 喔 ！ 在 是 名詞 。 後 面 的   gives   則是 名詞 子句 的 動詞 。\", \"you ' ll   need   to   pop   in   to   your   nearest   Medicare   service   center ,   fill   out   some   forms     話 為 何中間 沒有 連接 詞 ? ? ?   是   A ,   B ,   and / as   well   as   C .   的 句型 。 當列 舉三個 以上 的 事情 時 ， 只 在 最後一項 前面 連接 詞 即可 喔 。     列舉 三個 動作 ：   1 .   pop   in   to   your   nearest   Medicare   service   center   2 .   fill   out   some   forms   3 .   provide   some   sort   of   ID     因此 連接 詞 是 接 在   provide   some   sort   of   ID   之前 。 那 similar   to   ......... bank   account ， 不算 在 列舉 的 事情 裡面 嗎 ? ? 算名 詞 子句 ? ? 前面 並 列舉 的 東西 喔 ！   similar   的 用法 是   similar   to   something ... 。   setting   up   your   bank   account   是 名 詞片語 ， 可以 參考 以下 字典 連結 喔 ：   https : / / tw . dictionary . yahoo . com / dictionary ? p = similar + to\", \"  when 在 的 詞性 是 副 詞 連接 詞及 是 ，   在 是 連接 詞 的 角色 ， 引領 名詞 子句   when   we   knew   this   was   way   bigger   than   anything   we   had   imagined 。 另外 也 可以 參考 以下 字典 的 例子 ：     … 的   That   was   when   it   all   went   wrong .       那時 一切 都 亂 了 套   That ' s   when   I   was   born .       那 是 我 出生 的 那天   Now   is   when   we   must   act .       我們 現在 必須 行動 了\"]\n"
     ]
    }
   ],
   "source": [
    "X_train_text, y_train, X_test_text, y_test, member_train, member_test, question_train, question_test = dh.get_fixed_data()\n",
    "\n",
    "print(X_train_text[:3])\n",
    "print(X_test_text[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf = TextFeature(X_train_text, X_test_text)\n",
    "X_train, X_test = tf.get_tfidf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Naive Bayes corss validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/model_selection/_split.py:581: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "NB_cv = Pipeline([('cls', MultinomialNB()),])\n",
    "parameters = {'cls__alpha': (0.5, 0.8, 1.0, 5, 10)}\n",
    "gs_cls = GridSearchCV(NB_cv, param_grid = parameters, cv = 10, n_jobs = mp.cpu_count()-1)\n",
    "gs_cls = gs_cls.fit(X_train.todense(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Paras: {'cls__alpha': 0.5}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         2\n",
      "          1       1.00      0.83      0.90       207\n",
      "         10       0.89      0.92      0.91       528\n",
      "         11       0.92      0.19      0.32        57\n",
      "         12       0.91      0.94      0.93       515\n",
      "          2       0.96      0.80      0.87       193\n",
      "          3       0.92      0.85      0.88       448\n",
      "          4       0.00      0.00      0.00        35\n",
      "          5       0.92      0.94      0.93       462\n",
      "          6       1.00      0.09      0.17        96\n",
      "          7       1.00      0.11      0.20        88\n",
      "          8       0.98      0.50      0.66       244\n",
      "          9       0.70      0.99      0.82      1000\n",
      "\n",
      "avg / total       0.86      0.84      0.82      3875\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print('Best Paras:', gs_cls.best_params_)\n",
    "y_predict = gs_cls.predict(X_train)\n",
    "y_predict_prob = gs_cls.predict_proba(X_train)\n",
    "infile = 'predicted/NB_unamb_predict.csv'\n",
    "cat = [Grammar[item] for item in gs_cls.best_estimator_.classes_]\n",
    "with open(infile, 'w') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter = ',', quotechar = '\"')\n",
    "    spamwriter.writerow(['question_id', 'member_id', 'question'] + cat)\n",
    "    for i in range(len(question_train)):\n",
    "        writestring = [question_train[i], data_dict[question_train[i]]['member_id'], data_dict[question_train[i]]['question']]\n",
    "        writestring += list(y_predict_prob[i])\n",
    "        spamwriter.writerow(writestring)\n",
    "        \n",
    "print(metrics.classification_report(y_train, y_predict))\n",
    "\n",
    "y_predict_prob = gs_cls.predict_proba(X_test)\n",
    "infile = 'predicted/NB_amb_predict.csv'\n",
    "with open(infile, 'w') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter = ',', quotechar = '\"')\n",
    "    spamwriter.writerow(['question_id', 'member_id', 'question'] + cat)\n",
    "    for i in range(len(question_test)):\n",
    "        writestring = [question_test[i], data_dict[question_test[i]]['member_id'], data_dict[question_test[i]]['question']]\n",
    "        writestring += list(y_predict_prob[i])\n",
    "        spamwriter.writerow(writestring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Random Forest cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/model_selection/_split.py:581: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "RF_cv = Pipeline([('cls', RandomForestClassifier()),])\n",
    "parameters = {'cls__n_estimators': (20, 64, 128, 256),\n",
    "              'cls__max_features': ['auto', 'sqrt', 'log2']}\n",
    "gs_cls = GridSearchCV(RF_cv, param_grid = parameters, cv = 10, n_jobs = mp.cpu_count()-1)\n",
    "gs_cls = gs_cls.fit(X_train.todense(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('Best Paras:', gs_cls.best_params_)\n",
    "y_predict = gs_cls.predict(X_train)\n",
    "y_predict_prob = gs_cls.predict_proba(X_train)\n",
    "\n",
    "infile = 'predicted/RF_unamb_predict.csv'\n",
    "with open(infile, 'w') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter = ',', quotechar = '\"')\n",
    "    spamwriter.writerow(['question_id', 'member_id', 'question'] + cat)\n",
    "    for i in range(len(question_train)):\n",
    "        writestring = [question_train[i], data_dict[question_train[i]]['member_id'], data_dict[question_train[i]]['question']]\n",
    "        writestring += list(y_predict_prob[i])\n",
    "        spamwriter.writerow(writestring)\n",
    "        \n",
    "print(metrics.classification_report(y_train, y_predict))\n",
    "\n",
    "y_predict_prob = gs_cls.predict_proba(X_test)\n",
    "infile = 'predicted/RF_amb_predict.csv'\n",
    "with open(infile, 'w') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter = ',', quotechar = '\"')\n",
    "    spamwriter.writerow(['question_id', 'member_id', 'question'] + cat)\n",
    "    for i in range(len(question_test)):\n",
    "        writestring = [question_test[i], data_dict[question_test[i]]['member_id'], data_dict[question_test[i]]['question']]\n",
    "        writestring += list(y_predict_prob[i])\n",
    "        spamwriter.writerow(writestring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVM_cv = Pipeline([('cls', SVC()),])\n",
    "parameters = {'clf__kernel': ('linear', 'rbf', 'sigmoid'),\n",
    "              'clf__C': (0.01, 0.1, 1.0, 5, 10),\n",
    "              'clf__gamma': ('auto', 0.1, 1, 10)}\n",
    "gs_cls = GridSearchCV(SVM_cv, param_grid = parameters, cv = 10, n_jobs = mp.cpu_count()-1)\n",
    "gs_cls = gs_cls.fit(X_train.todense(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Best Paras:', gs_cls.best_params_)\n",
    "y_predict = gs_cls.predict(X_train)\n",
    "y_predict_prob = gs_cls.predict_proba(X_train)\n",
    "\n",
    "infile = 'predicted/SVM_unamb_predict.csv'\n",
    "with open(infile, 'w') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter = ',', quotechar = '\"')\n",
    "    spamwriter.writerow(['question_id', 'member_id', 'question'] + cat)\n",
    "    for i in range(len(question_train)):\n",
    "        writestring = [question_train[i], data_dict[question_train[i]]['member_id'], data_dict[question_train[i]]['question']]\n",
    "        writestring += list(y_predict_prob[i])\n",
    "        spamwriter.writerow(writestring)\n",
    "        \n",
    "print(metrics.classification_report(y_train, y_predict))\n",
    "\n",
    "\n",
    "y_predict_prob = gs_cls.predict_proba(X_test)\n",
    "infile = 'predicted/SVM_unamb_predict.csv'\n",
    "with open(infile, 'w') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter = ',', quotechar = '\"')\n",
    "    spamwriter.writerow(['question_id', 'member_id', 'question'] + cat)\n",
    "    for i in range(len(question_test)):\n",
    "        writestring = [question_test[i], data_dict[question_test[i]]['member_id'], data_dict[question_test[i]]['question']]\n",
    "        writestring += list(y_predict_prob[i])\n",
    "        spamwriter.writerow(writestring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         1\n",
      "          1       0.96      0.55      0.70        42\n",
      "         10       0.90      0.65      0.75       106\n",
      "         11       0.00      0.00      0.00        12\n",
      "         12       0.83      0.84      0.84       103\n",
      "          2       0.87      0.51      0.65        39\n",
      "          3       0.86      0.71      0.78        90\n",
      "          4       0.00      0.00      0.00         7\n",
      "          5       0.87      0.81      0.84        93\n",
      "          6       0.00      0.00      0.00        20\n",
      "          7       0.00      0.00      0.00        18\n",
      "          8       1.00      0.20      0.34        49\n",
      "          9       0.52      0.99      0.69       200\n",
      "\n",
      "avg / total       0.73      0.70      0.67       780\n",
      "\n",
      "780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "NB = MultinomialNB(alpha = 1.0)\n",
    "NB.fit(X_train.todense(), y_train)\n",
    "y_predict = NB.predict(X_test.todense())\n",
    "print(metrics.classification_report(y_test, y_predict))\n",
    "\n",
    "y_predict_prob = NB.predict_proba(X_test.todense())\n",
    "infile = 'predicted/NB_question_predict.csv'\n",
    "with open(infile, 'w') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter = ',', quotechar = '\"')\n",
    "    spamwriter.writerow(['question_id', 'member_id', 'question'] + cat)\n",
    "    print(len(question_test))\n",
    "    for i in range(len(question_test)):\n",
    "        writestring = [question_test[i], data_dict[question_test[i]]['member_id'], data_dict[question_test[i]]['question']]\n",
    "        writestring += list(y_predict_prob[i])\n",
    "        spamwriter.writerow(writestring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         1\n",
      "          1       1.00      0.95      0.98        42\n",
      "         10       0.96      0.93      0.95       106\n",
      "         11       1.00      0.83      0.91        12\n",
      "         12       0.95      0.97      0.96       103\n",
      "          2       0.97      0.87      0.92        39\n",
      "          3       0.90      0.88      0.89        90\n",
      "          4       1.00      0.57      0.73         7\n",
      "          5       0.87      0.99      0.92        93\n",
      "          6       1.00      0.85      0.92        20\n",
      "          7       1.00      0.61      0.76        18\n",
      "          8       0.95      0.84      0.89        49\n",
      "          9       0.87      0.95      0.91       200\n",
      "\n",
      "avg / total       0.92      0.92      0.92       780\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "RF  = RandomForestClassifier(n_jobs=-1, max_features=\"sqrt\", n_estimators=128)\n",
    "RF.fit(X_train.todense(), y_train)\n",
    "y_predicted = RF.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         1\n",
      "          1       1.00      1.00      1.00        42\n",
      "         10       0.95      0.94      0.95       106\n",
      "         11       1.00      0.67      0.80        12\n",
      "         12       0.93      0.96      0.94       103\n",
      "          2       0.95      0.92      0.94        39\n",
      "          3       0.93      0.91      0.92        90\n",
      "          4       1.00      1.00      1.00         7\n",
      "          5       0.93      0.97      0.95        93\n",
      "          6       0.90      0.95      0.93        20\n",
      "          7       0.80      0.67      0.73        18\n",
      "          8       0.95      0.86      0.90        49\n",
      "          9       0.92      0.95      0.94       200\n",
      "\n",
      "avg / total       0.93      0.93      0.93       780\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "svc = LinearSVC(C=1.0, max_iter=10000)\n",
    "svc = svc.fit(X = X_train.todense(), y = y_train)\n",
    "y_predict = svc.predict(X = X_test)\n",
    "print(metrics.classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### get fixed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape: 3875\n",
      "y train shape: (3875,)\n"
     ]
    }
   ],
   "source": [
    "X_train_text, y_train, X_test_text, y_test, member_train, member_test, question_train, question_test = dh.get_fixed_data()\n",
    "print('X train shape: {}'.format(len(X_train_text)))\n",
    "print('y train shape: {}'.format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3875, 10760)\n",
      "(1949, 10760)\n"
     ]
    }
   ],
   "source": [
    "tf = TextFeature(X_train_text, X_test_text)\n",
    "X_train, X_test = tf.get_tfidf()\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## write predicted results into file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "NB = MultinomialNB(alpha = 1.0)\n",
    "NB.fit(X_train.todense(), y_train)\n",
    "y_predict_prob = NB.predict_proba(X_test.todense())\n",
    "cat = [Grammar[item] for item in NB.classes_]\n",
    "out_NB = 'predicted/NB_predicted.csv'\n",
    "with open(out_NB, 'w') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter = ',', quotechar = '\"')\n",
    "    spamwriter.writerow(['question_id', 'member_id', 'question'] + cat)\n",
    "    for i in range(len(question_test)):\n",
    "        writestring = [question_test[i], data_dict[question_test[i]]['member_id'], data_dict[question_test[i]]['question']]\n",
    "        writestring += list(y_predict_prob[i])\n",
    "        spamwriter.writerow(writestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "RF  = RandomForestClassifier(n_jobs=-1, max_features=\"sqrt\", n_estimators=128)\n",
    "RF.fit(X_train.todense(), y_train)\n",
    "y_predict_prob = RF.predict_proba(X_test.todense())\n",
    "cat = [Grammar[item] for item in RF.classes_]\n",
    "out_RF = 'predicted/RF_predicted.csv'\n",
    "with open(out_RF, 'w') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter = ',', quotechar = '\"')\n",
    "    spamwriter.writerow(['question_id', 'member_id', 'question'] + cat)\n",
    "    for i in range(len(question_test)):\n",
    "        writestring = [question_test[i], data_dict[question_test[i]]['member_id'], data_dict[question_test[i]]['question']]\n",
    "        writestring += list(y_predict_prob[i])\n",
    "        spamwriter.writerow(writestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "SVC = LinearSVC(C=1.0, max_iter=10000)\n",
    "SVC = SVC.fit(X = X_train.todense(), y = y_train)\n",
    "y_predict = SVC.predict(X_test.todense())\n",
    "# cat = [Grammar[item] for item in SVC.classes_]\n",
    "out_SVC = 'predicted/SVC_predicted.csv'\n",
    "with open(out_SVC, 'w') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter = ',', quotechar = '\"')\n",
    "    spamwriter.writerow(['question_id', 'member_id', 'question'] + cat)\n",
    "    for i in range(len(question_test)):\n",
    "        writestring = [question_test[i], data_dict[question_test[i]]['member_id'], data_dict[question_test[i]]['question']]\n",
    "        writestring += [Grammar[y_predict[i]]]\n",
    "        spamwriter.writerow(writestring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Convert csv to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "def conver2json(infile, outfile = 'predicted/NB_question_predict.json'):\n",
    "    predict_dict = defaultdict(dict)\n",
    "\n",
    "    predict_dict = defaultdict(dict)\n",
    "    with open(infile, 'r') as csvfile:\n",
    "        for row in csv.DictReader(csvfile):\n",
    "            predict_dict[row['question_id']]['member_id'] = row['member_id']\n",
    "            predict_dict[row['question_id']]['question'] = row['question']\n",
    "            predict_dict[row['question_id']]['reply'] = 'NO!'\n",
    "            predict_dict[row['question_id']]['其它'] = row['其它']\n",
    "            predict_dict[row['question_id']]['完成式'] = row['完成式']\n",
    "            predict_dict[row['question_id']]['連接詞'] = row['連接詞']\n",
    "            predict_dict[row['question_id']]['假設語氣'] = row['假設語氣']\n",
    "            predict_dict[row['question_id']]['分詞'] = row['分詞']\n",
    "            predict_dict[row['question_id']]['進行式'] = row['進行式']\n",
    "            predict_dict[row['question_id']]['過去式'] = row['過去式']\n",
    "            predict_dict[row['question_id']]['未來式'] = row['未來式']\n",
    "            predict_dict[row['question_id']]['關係代名詞'] = row['關係代名詞']\n",
    "            predict_dict[row['question_id']]['不定詞'] = row['不定詞']\n",
    "            predict_dict[row['question_id']]['名詞子句'] = row['名詞子句']\n",
    "            predict_dict[row['question_id']]['被動'] = row['被動']\n",
    "            predict_dict[row['question_id']]['介係詞'] = row['介係詞']\n",
    "            predict_dict[row['question_id']]['question_type'] = 0\n",
    "    \n",
    "    with open(outfile, 'w') as jsonfile:\n",
    "        json.dump(predict_dict, jsonfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "infile = 'predicted/NB_question_predicted.csv'\n",
    "outfile = 'predicted/NB_question_predicted.json'\n",
    "conver2json(infile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
