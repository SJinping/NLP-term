{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Classifier with TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import jieba\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## type dict\n",
    "Grammar = {'完成式': 1, '進行式': 2, '過去式': 3, '未來式': 4, '關係代名詞': 5, '不定詞': 6, '名詞子句': 7, \n",
    "           '被動': 8, '介係詞': 9, '連接詞': 10, '假設語氣': 11, '現在分詞': 12, '過去分詞': 13, 'PT': 14, '其它': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataHelper(object):\n",
    "    def __init__(self, file):\n",
    "        self.file = file\n",
    "        self.stopwords = ['什麼', '請問', '這裡', '不是', '意思', '這邊', '謝謝', '這句', '為何', '使用', '怎麼', '要加', '老師', '還是', '如何', '甚麼', '一下', '這個', '這樣', '問為', '因為', '何要', '用過', '是不是', '一個', '應該', '直接', '好像', '如果', '何不', '兩個', '這是', '何用', '需要', '時候', '所以', '您好', '起來', '還有', '加上', '寫成', '你好', '此句', '有點', '問此', '不好意思', '不到', '像是', '這裏', '為什麼']\n",
    "        \n",
    "        with open('{0}'.format(self.file)) as data_file:\n",
    "            self.data = {}\n",
    "            for row in csv.DictReader(data_file):\n",
    "                if row['type'] in self.data:\n",
    "                    self.data[row['type']].append(row.values())\n",
    "                else:\n",
    "                    self.data[row['type']] = []\n",
    "                    \n",
    "        data_file.close()\n",
    "        \n",
    "    def get_shuffled_data(self, ratio = 8):\n",
    "        X_train = []\n",
    "        X_test = []\n",
    "        Y_train = []\n",
    "        Y_test = []\n",
    "        member_train = []\n",
    "        member_test = []\n",
    "        for key, record in self.data.items():\n",
    "            if key == '14':\n",
    "                continue\n",
    "            questions = list(list(zip(*record))[2]) # get question list from records\n",
    "            members = list(list(zip(*record))[1]) # get memberid list from records\n",
    "            random.shuffle(questions)\n",
    "            split_point = len(questions)*ratio//10\n",
    "            train = questions[:split_point]\n",
    "            test = questions[split_point:]\n",
    "            member_train += members[:split_point]\n",
    "            member_test += members[split_point:]\n",
    "            X_train += train\n",
    "            X_test += test\n",
    "            Y_train += [key]*len(train)\n",
    "            Y_test += [key]*len(test)\n",
    "            \n",
    "        X_train_text = self.cut_questions(X_train)\n",
    "        X_test_text = self.cut_questions(X_test)\n",
    "        return X_train_text, np.array(Y_train), X_test_text, np.array(Y_test), member_train, member_test\n",
    "    \n",
    "    # use non-duplications as training and duplications as testing\n",
    "    # the file should be questions_nondup_dup.csv\n",
    "    def get_fixed_data(self):\n",
    "        X_train = []\n",
    "        X_test = []\n",
    "        Y_train = []\n",
    "        Y_test = []\n",
    "        member_train = []\n",
    "        member_test = []\n",
    "        try:\n",
    "            for key, record in self.data.items():\n",
    "                if int(record['dup']) == 0: # non-duplications\n",
    "                    X_train.append(record['question'])\n",
    "                    Y_train.append(record['type'])\n",
    "                    member_train.append(record['member_id'])\n",
    "                else:\n",
    "                    X_test.append(record['question'])\n",
    "                    Y_test.append(record['type'])\n",
    "                    member_test.append(record['member_id'])\n",
    "        except KeyError as e:\n",
    "            print(e)\n",
    "            raise\n",
    "            \n",
    "        X_train_text = self.cut_questions(self.X_train)\n",
    "        X_test_text = self.cut_questions(self.X_test)\n",
    "        return X_train_text, np.array(Y_train), X_text_text, np.array(Y_test), member_train, member_test\n",
    "        \n",
    "    def cut_questions(self, data):\n",
    "        corpus = []\n",
    "        for q in data:\n",
    "            segs = jieba.cut(q, cut_all=False)\n",
    "            final = [seg for seg in segs if seg not in self.stopwords]\n",
    "            corpus.append(' '.join(final))\n",
    "        return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y train shape: (3058,)\n",
      "y test shape: (773,)\n"
     ]
    }
   ],
   "source": [
    "dh = DataHelper('questions_nondup.csv')\n",
    "X_train_text, y_train, X_test_text, y_test, member_train, member_test = dh.get_shuffled_data()\n",
    "print('y train shape: {}'.format(y_train.shape))\n",
    "print('y test shape: {}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract features of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextFeature(object):\n",
    "    def __init__(self, training_data, testing_data):\n",
    "        self.training_text = training_data\n",
    "        self.testing_text = testing_data\n",
    "        \n",
    "    def get_tfidf(self, use_idf = True):\n",
    "#         texts = self.training_text + self.testing_text\n",
    "        tfidf_vectorizer = TfidfVectorizer(use_idf = True)\n",
    "        tfidf_vectorizer.fit(self.training_text)\n",
    "        joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')\n",
    "        X_train = tfidf_vectorizer.transform(self.training_text)\n",
    "        X_test = tfidf_vectorizer.transform(self.testing_text)\n",
    "        return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3058, 4617)\n",
      "(773, 4617)\n"
     ]
    }
   ],
   "source": [
    "tf = TextFeature(X_train_text, X_test_text)\n",
    "X_train, X_test = tf.get_tfidf()\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.95      0.50      0.66        42\n",
      "         10       0.74      0.61      0.67       106\n",
      "         11       1.00      0.09      0.17        11\n",
      "         12       1.00      0.22      0.36        59\n",
      "         13       0.88      0.17      0.28        42\n",
      "          2       1.00      0.26      0.41        39\n",
      "          3       0.71      0.52      0.60        90\n",
      "          4       0.00      0.00      0.00         7\n",
      "          5       0.80      0.45      0.58        91\n",
      "          6       0.00      0.00      0.00        19\n",
      "          7       0.00      0.00      0.00        18\n",
      "          8       0.67      0.08      0.15        49\n",
      "          9       0.39      0.99      0.56       200\n",
      "\n",
      "avg / total       0.66      0.53      0.49       773\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['naive_bayes.pkl']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB = MultinomialNB(alpha = 1.0)\n",
    "NB.fit(X_train.todense(), y_train)\n",
    "y_predict = NB.predict(X_test.todense())\n",
    "print(metrics.classification_report(y_test, y_predict))\n",
    "joblib.dump(NB, 'naive_bayes.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.77      0.79      0.78        42\n",
      "         10       0.72      0.73      0.72       106\n",
      "         11       1.00      0.27      0.43        11\n",
      "         12       0.58      0.49      0.53        59\n",
      "         13       0.69      0.43      0.53        42\n",
      "          2       0.96      0.69      0.81        39\n",
      "          3       0.79      0.58      0.67        90\n",
      "          4       0.86      0.86      0.86         7\n",
      "          5       0.72      0.68      0.70        91\n",
      "          6       1.00      0.58      0.73        19\n",
      "          7       0.71      0.56      0.63        18\n",
      "          8       0.84      0.63      0.72        49\n",
      "          9       0.60      0.89      0.72       200\n",
      "\n",
      "avg / total       0.72      0.69      0.69       773\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['random_forest.pkl']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF  = RandomForestClassifier(n_jobs=-1, max_features=\"sqrt\", n_estimators=256)\n",
    "RF.fit(X_train.todense(), y_train)\n",
    "y_predicted = RF.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_predicted))\n",
    "joblib.dump(RF, 'random_forest.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.81      0.81      0.81        42\n",
      "         10       0.70      0.74      0.72       106\n",
      "         11       0.71      0.45      0.56        11\n",
      "         12       0.72      0.61      0.66        59\n",
      "         13       0.53      0.45      0.49        42\n",
      "          2       0.97      0.72      0.82        39\n",
      "          3       0.76      0.70      0.73        90\n",
      "          4       0.75      0.86      0.80         7\n",
      "          5       0.70      0.71      0.71        91\n",
      "          6       0.92      0.58      0.71        19\n",
      "          7       0.67      0.44      0.53        18\n",
      "          8       0.80      0.65      0.72        49\n",
      "          9       0.68      0.85      0.76       200\n",
      "\n",
      "avg / total       0.72      0.72      0.72       773\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['svc.pkl']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = LinearSVC(C=1.0, max_iter=10000)\n",
    "svc = svc.fit(X = X_train.todense(), y = y_train)\n",
    "y_predict = svc.predict(X = X_test)\n",
    "print(metrics.classification_report(y_test, y_predict))\n",
    "joblib.dump(svc, 'svc.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
