{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Train Classifier with TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import jieba\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## type dict\n",
    "Grammar = {'完成式': 1, '進行式': 2, '過去式': 3, '未來式': 4, '關係代名詞': 5, '不定詞': 6, '名詞子句': 7, \n",
    "           '被動': 8, '介係詞': 9, '連接詞': 10, '假設語氣': 11, '分詞': 12, 'PT': 13, '其它': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class DataHelper(object):\n",
    "    def __init__(self, file):\n",
    "        self.file = file\n",
    "        self.stopwords = ['什麼', '請問', '這裡', '不是', '意思', '這邊', '謝謝', '這句', '為何', '使用', '怎麼', '要加', '老師', '還是', '如何', '甚麼', '一下', '這個', '這樣', '問為', '因為', '何要', '用過', '是不是', '一個', '應該', '直接', '好像', '如果', '何不', '兩個', '這是', '何用', '需要', '時候', '所以', '您好', '起來', '還有', '加上', '寫成', '你好', '此句', '有點', '問此', '不好意思', '不到', '像是', '這裏', '為什麼']\n",
    "        \n",
    "        with open('{0}'.format(self.file)) as data_file:\n",
    "            self.unamb_data = defaultdict(list)\n",
    "            self.amb_data = defaultdict(list)\n",
    "            for row in csv.DictReader(data_file):\n",
    "                if row['ambiguous'] == '0':\n",
    "                    self.unamb_data[row['type']].append(row.values())\n",
    "                else:\n",
    "                    self.amb_data[row['type']].append(row.values())\n",
    "        \n",
    "    def get_shuffled_data(self, ratio = 8):\n",
    "        X_train = []\n",
    "        X_test = []\n",
    "        Y_train = []\n",
    "        Y_test = []\n",
    "        member_train = []\n",
    "        member_test = []\n",
    "#         question_train = []\n",
    "#         question_test = []\n",
    "        for key, record in self.unamb_data.items():\n",
    "            if key == '13':\n",
    "                continue\n",
    "            questions = list(list(zip(*record))[3]) # get question list from records\n",
    "            members = list(list(zip(*record))[1]) # get memberid list from records\n",
    "            question_idx = list(list(zip(*record))[0])\n",
    "            random.shuffle(questions)\n",
    "            split_point = len(questions)*ratio//10\n",
    "            train = questions[:split_point]\n",
    "            test = questions[split_point:]\n",
    "            member_train += members[:split_point]\n",
    "            member_test += members[split_point:]\n",
    "#             question_train += question_idx[:split_point]\n",
    "#             question_train += question_idx[split_point:]\n",
    "            X_train += train\n",
    "            X_test += test\n",
    "            Y_train += [key]*len(train) # repeat len(train) times\n",
    "            Y_test += [key]*len(test)\n",
    "            \n",
    "        X_train_text = self.cut_questions(X_train)\n",
    "        X_test_text = self.cut_questions(X_test)\n",
    "        return X_train_text, np.array(Y_train), X_test_text, np.array(Y_test), member_train, member_test\n",
    "    \n",
    "    # use non-duplications as training and duplications as testing\n",
    "    # the file should be questions_nondup_dup.csv\n",
    "    def get_fixed_data(self):\n",
    "        X_train = []\n",
    "        X_test = []\n",
    "        Y_train = []\n",
    "        Y_test = []\n",
    "        member_train = []\n",
    "        member_test = []\n",
    "        question_train = []\n",
    "        question_test = []\n",
    "        \n",
    "        for key, record in self.unamb_data.items():\n",
    "            questions = list(list(zip(*record))[3]) # get question list from records\n",
    "            members = list(list(zip(*record))[1]) # get memberid list from records\n",
    "            question_idx = list(list(zip(*record))[0])\n",
    "            X_train += questions\n",
    "            Y_train += [key]*len(questions)\n",
    "            member_train += members\n",
    "            question_train += question_idx\n",
    "        for key, record in self.amb_data.items():\n",
    "            questions = list(list(zip(*record))[3]) # get question list from records\n",
    "            members = list(list(zip(*record))[1]) # get memberid list from records\n",
    "            question_idx = list(list(zip(*record))[0])\n",
    "            X_test += questions\n",
    "            Y_test += [key]*len(questions)\n",
    "            member_test += members\n",
    "            question_test += question_idx\n",
    "            \n",
    "        X_train_text = self.cut_questions(X_train)\n",
    "        X_test_text = self.cut_questions(X_test)\n",
    "        return X_train_text, np.array(Y_train), X_test_text, np.array(Y_test), member_train, member_test, question_train, question_test\n",
    "        \n",
    "    def cut_questions(self, data):\n",
    "        corpus = []\n",
    "        for q in data:\n",
    "            segs = jieba.cut(q, cut_all=False)\n",
    "            final = [seg for seg in segs if seg not in self.stopwords]\n",
    "            corpus.append(' '.join(final))\n",
    "        return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dict_values(['30383', '56291', '2', '這裡的 letting 加ing是因為also的關係嗎?  You wanna be getting to know a person 是未來進行式吧?', '0']), dict_values(['28854', '56291', '2', \"you'll be speaking 為甚麼speak要加ing 還有這句要如何解釋\", '0']), dict_values(['23656', '78952', '2', '請問My day started out walking ~walking為什麼是現在進行式?另外可否不用start out片語,用My day start to walk with my dog來表示是正確的嗎?', '0'])]\n",
      "[dict_values(['32440', '88024', '2', '為什麼wanna後面要用be getting而不是直接用wanna get to ... ??', '1']), dict_values(['29663', '56291', '2', ' be making friends 這裡是未來進行式嗎?   throughout 這裡昰介係詞嗎', '1']), dict_values(['39673', '84855', '2', \"可以寫成這樣嗎？ we're going to make friends, meet people, introduce ourselves to others.\", '1'])]\n"
     ]
    }
   ],
   "source": [
    "dh = DataHelper('questions_nondup_dup.csv')\n",
    "print(dh.unamb_data['2'][:3])\n",
    "print(dh.amb_data['2'][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get shuffled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape: (3095, 4644)\n",
      "y train shape: (3095,)\n"
     ]
    }
   ],
   "source": [
    "X_train_text, y_train, X_test_text, y_test, member_train, member_test = dh.get_shuffled_data()\n",
    "print('X train shape: {}'.format(X_train.shape))\n",
    "print('y train shape: {}'.format(y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## extract features of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class TextFeature(object):\n",
    "    def __init__(self, training_data, testing_data):\n",
    "        self.training_text = training_data\n",
    "        self.testing_text = testing_data\n",
    "        \n",
    "    def get_tfidf(self, use_idf = True):\n",
    "#         texts = self.training_text + self.testing_text\n",
    "        tfidf_vectorizer = TfidfVectorizer(use_idf = True)\n",
    "        tfidf_vectorizer.fit(self.training_text)\n",
    "        X_train = tfidf_vectorizer.transform(self.training_text)\n",
    "        X_test = tfidf_vectorizer.transform(self.testing_text)\n",
    "        return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## get features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3095, 4644)\n",
      "(780, 4644)\n"
     ]
    }
   ],
   "source": [
    "tf = TextFeature(X_train_text, X_test_text)\n",
    "X_train, X_test = tf.get_tfidf()\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         1\n",
      "          1       1.00      0.38      0.55        42\n",
      "         10       0.71      0.56      0.62       106\n",
      "         11       0.00      0.00      0.00        12\n",
      "         12       0.67      0.40      0.50       103\n",
      "          2       1.00      0.23      0.38        39\n",
      "          3       0.71      0.47      0.56        90\n",
      "          4       0.00      0.00      0.00         7\n",
      "          5       0.72      0.45      0.56        93\n",
      "          6       0.00      0.00      0.00        20\n",
      "          7       0.00      0.00      0.00        18\n",
      "          8       0.75      0.06      0.11        49\n",
      "          9       0.40      0.98      0.57       200\n",
      "\n",
      "avg / total       0.61      0.52      0.48       780\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "NB = MultinomialNB(alpha = 1.0)\n",
    "NB.fit(X_train.todense(), y_train)\n",
    "y_predict = NB.predict(X_test.todense())\n",
    "print(metrics.classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         1\n",
      "          1       0.78      0.74      0.76        42\n",
      "         10       0.72      0.71      0.71       106\n",
      "         11       0.90      0.75      0.82        12\n",
      "         12       0.63      0.46      0.53       103\n",
      "          2       0.78      0.72      0.75        39\n",
      "          3       0.73      0.71      0.72        90\n",
      "          4       0.75      0.43      0.55         7\n",
      "          5       0.73      0.65      0.69        93\n",
      "          6       1.00      0.40      0.57        20\n",
      "          7       0.61      0.61      0.61        18\n",
      "          8       0.82      0.55      0.66        49\n",
      "          9       0.62      0.87      0.72       200\n",
      "\n",
      "avg / total       0.70      0.69      0.68       780\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "RF  = RandomForestClassifier(n_jobs=-1, max_features=\"sqrt\", n_estimators=128)\n",
    "RF.fit(X_train.todense(), y_train)\n",
    "y_predicted = RF.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.81      0.83      0.82        42\n",
      "         10       0.70      0.74      0.72       106\n",
      "         11       1.00      0.55      0.71        11\n",
      "         12       0.65      0.61      0.63        59\n",
      "         13       0.62      0.36      0.45        42\n",
      "          2       0.74      0.72      0.73        39\n",
      "          3       0.68      0.74      0.71        90\n",
      "          4       1.00      0.57      0.73         7\n",
      "          5       0.75      0.75      0.75        91\n",
      "          6       0.69      0.47      0.56        19\n",
      "          7       0.71      0.67      0.69        18\n",
      "          8       0.82      0.63      0.71        49\n",
      "          9       0.71      0.82      0.76       200\n",
      "\n",
      "avg / total       0.72      0.72      0.71       773\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc = LinearSVC(C=1.0, max_iter=10000)\n",
    "svc = svc.fit(X = X_train.todense(), y = y_train)\n",
    "y_predict = svc.predict(X = X_test)\n",
    "print(metrics.classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get fixed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape: 6744\n",
      "y train shape: (6744,)\n"
     ]
    }
   ],
   "source": [
    "X_train_text, y_train, X_test_text, y_test, member_train, member_test, question_train, question_test = dh.get_fixed_data()\n",
    "print('X train shape: {}'.format(len(X_train_text)))\n",
    "print('y train shape: {}'.format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6744, 9539)\n",
      "(1949, 9539)\n"
     ]
    }
   ],
   "source": [
    "tf = TextFeature(X_train_text, X_test_text)\n",
    "X_train, X_test = tf.get_tfidf()\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write predicted results into file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Grammar = {'1': '完成式', '2': '進行式', '3': '過去式', '4': '未來式', '5': '關係代名詞', '6': '不定詞', '7': '名詞子句', '8': '被動', '9': '介係詞', \\\n",
    "           '10': '連接詞', '11': '假設語氣', '12': '分詞', '13': 'PT', '0': '其它'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('questions_nondup_dup.csv') as csvfile:\n",
    "    data_dict = defaultdict()\n",
    "    for row in csv.DictReader(csvfile):\n",
    "        data_dict[row['question_id']] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NB = MultinomialNB(alpha = 1.0)\n",
    "NB.fit(X_train.todense(), y_train)\n",
    "y_predict_prob = NB.predict_proba(X_test.todense())\n",
    "cat = [Grammar[item] for item in NB.classes_]\n",
    "out_NB = 'predicted/NB_predicted.csv'\n",
    "with open(out_NB, 'w') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter = ',', quotechar = '\"')\n",
    "    spamwriter.writerow(['question_id', 'member_id', 'question'] + cat)\n",
    "    for i in range(len(question_test)):\n",
    "        writestring = [question_test[i], data_dict[question_test[i]]['member_id'], data_dict[question_test[i]]['question']]\n",
    "        writestring += list(y_predict_prob[i])\n",
    "        spamwriter.writerow(writestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RF  = RandomForestClassifier(n_jobs=-1, max_features=\"sqrt\", n_estimators=128)\n",
    "RF.fit(X_train.todense(), y_train)\n",
    "y_predict_prob = RF.predict_proba(X_test.todense())\n",
    "cat = [Grammar[item] for item in RF.classes_]\n",
    "out_RF = 'predicted/RF_predicted.csv'\n",
    "with open(out_RF, 'w') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter = ',', quotechar = '\"')\n",
    "    spamwriter.writerow(['question_id', 'member_id', 'question'] + cat)\n",
    "    for i in range(len(question_test)):\n",
    "        writestring = [question_test[i], data_dict[question_test[i]]['member_id'], data_dict[question_test[i]]['question']]\n",
    "        writestring += list(y_predict_prob[i])\n",
    "        spamwriter.writerow(writestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SVC = LinearSVC(C=1.0, max_iter=10000)\n",
    "SVC = SVC.fit(X = X_train.todense(), y = y_train)\n",
    "y_predict = SVC.predict(X_test.todense())\n",
    "# cat = [Grammar[item] for item in SVC.classes_]\n",
    "out_SVC = 'predicted/SVC_predicted.csv'\n",
    "with open(out_SVC, 'w') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter = ',', quotechar = '\"')\n",
    "    spamwriter.writerow(['question_id', 'member_id', 'question'] + cat)\n",
    "    for i in range(len(question_test)):\n",
    "        writestring = [question_test[i], data_dict[question_test[i]]['member_id'], data_dict[question_test[i]]['question']]\n",
    "        writestring += [Grammar[y_predict[i]]]\n",
    "        spamwriter.writerow(writestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
