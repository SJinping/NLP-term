{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Train Classifier with TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import jieba\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## type dict\n",
    "Grammar = {'完成式': 1, '進行式': 2, '過去式': 3, '未來式': 4, '關係代名詞': 5, '不定詞': 6, '名詞子句': 7, \n",
    "           '被動': 8, '介係詞': 9, '連接詞': 10, '假設語氣': 11, '分詞': 12, 'PT': 13, '其它': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Grammar = {'1': '完成式', '2': '進行式', '3': '過去式', '4': '未來式', '5': '關係代名詞', '6': '不定詞', '7': '名詞子句', '8': '被動', '9': '介係詞', \\\n",
    "           '10': '連接詞', '11': '假設語氣', '12': '分詞', '13': 'PT', '0': '其它'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('questions_nondup_dup.csv') as csvfile:\n",
    "    data_dict = defaultdict()\n",
    "    for row in csv.DictReader(csvfile):\n",
    "        data_dict[row['question_id']] = row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class DataHelper(object):\n",
    "    def __init__(self, file):\n",
    "        self.file = file\n",
    "        self.stopwords = ['什麼', '請問', '這裡', '不是', '意思', '這邊', '謝謝', '這句', '為何', '使用', '怎麼', '要加', '老師', '還是', '如何', '甚麼', '一下', '這個', '這樣', '問為', '因為', '何要', '用過', '是不是', '一個', '應該', '直接', '好像', '如果', '何不', '兩個', '這是', '何用', '需要', '時候', '所以', '您好', '起來', '還有', '加上', '寫成', '你好', '此句', '有點', '問此', '不好意思', '不到', '像是', '這裏', '為什麼']\n",
    "        \n",
    "        with open('{0}'.format(self.file)) as data_file:\n",
    "            self.unamb_data = defaultdict(list)\n",
    "            self.amb_data = defaultdict(list)\n",
    "            for row in csv.DictReader(data_file):\n",
    "                if row['ambiguous'] == '0':\n",
    "                    # can't directly use row.values() as it doesn't grantee the order\n",
    "                    self.unamb_data[row['type']].append([row['question_id'], row['member_id'], \\\n",
    "                                                         row['type'], row['question'], row['ambiguous']])\n",
    "                else:\n",
    "                    self.amb_data[row['type']].append([row['question_id'], row['member_id'], \\\n",
    "                                                         row['type'], row['question'], row['ambiguous']])\n",
    "                    \n",
    "    def get_all_unambiguous_data(self):\n",
    "        X = []\n",
    "        y = []\n",
    "        member_id = []\n",
    "        question_id = []\n",
    "        for key, record in self.unamb_data.items():\n",
    "            if key == '13':\n",
    "                continue\n",
    "            questions = list(list(zip(*record))[3]) \n",
    "            members = list(list(zip(*record))[1]) # get memberid list from records\n",
    "            question_idx = list(list(zip(*record))[0])\n",
    "            X += questions\n",
    "            y += [key]*len(questions)\n",
    "            member_id += members\n",
    "            question_id += question_idx\n",
    "            \n",
    "        X_text = self.cut_questions(X)\n",
    "        return X_text, np.array(y), member_id, question_id\n",
    "        \n",
    "    def get_shuffled_data(self, ratio = 8):\n",
    "        X_train = []\n",
    "        X_test = []\n",
    "        Y_train = []\n",
    "        Y_test = []\n",
    "        member_train = []\n",
    "        member_test = []\n",
    "        question_train = []\n",
    "        question_test = []\n",
    "        for key, record in self.unamb_data.items():\n",
    "            if key == '13':\n",
    "                continue\n",
    "            questions = list(list(zip(*record))[3]) # get question list from records\n",
    "            members = list(list(zip(*record))[1]) # get memberid list from records\n",
    "            question_idx = list(list(zip(*record))[0])\n",
    "            random.shuffle(questions)\n",
    "            split_point = len(questions)*ratio//10\n",
    "            train = questions[:split_point]\n",
    "            test = questions[split_point:]\n",
    "            member_train += members[:split_point]\n",
    "            member_test += members[split_point:]\n",
    "            question_train += question_idx[:split_point]\n",
    "            question_test += question_idx[split_point:]\n",
    "            X_train += train\n",
    "            X_test += test\n",
    "            Y_train += [key]*len(train) # repeat len(train) times\n",
    "            Y_test += [key]*len(test)\n",
    "            \n",
    "        X_train_text = self.cut_questions(X_train)\n",
    "        X_test_text = self.cut_questions(X_test)\n",
    "        return X_train_text, np.array(Y_train), X_test_text, np.array(Y_test), member_train, member_test, question_train, question_test\n",
    "    \n",
    "    # use non-duplications as training and duplications as testing\n",
    "    # the file should be questions_nondup_dup.csv\n",
    "    def get_fixed_data(self):\n",
    "        X_train = []\n",
    "        X_test = []\n",
    "        Y_train = []\n",
    "        Y_test = []\n",
    "        member_train = []\n",
    "        member_test = []\n",
    "        question_train = []\n",
    "        question_test = []\n",
    "        \n",
    "        for key, record in self.unamb_data.items():\n",
    "            if key == '13':\n",
    "                continue\n",
    "            questions = list(list(zip(*record))[3]) # get question list from records\n",
    "            members = list(list(zip(*record))[1]) # get memberid list from records\n",
    "            question_idx = list(list(zip(*record))[0])\n",
    "            X_train += questions\n",
    "            Y_train += [key]*len(questions)\n",
    "            member_train += members\n",
    "            question_train += question_idx\n",
    "        for key, record in self.amb_data.items():\n",
    "            if key == '13':\n",
    "                continue\n",
    "            questions = list(list(zip(*record))[3]) # get question list from records\n",
    "            members = list(list(zip(*record))[1]) # get memberid list from records\n",
    "            question_idx = list(list(zip(*record))[0])\n",
    "            X_test += questions\n",
    "            Y_test += [key]*len(questions)\n",
    "            member_test += members\n",
    "            question_test += question_idx\n",
    "            \n",
    "        X_train_text = self.cut_questions(X_train)\n",
    "        X_test_text = self.cut_questions(X_test)\n",
    "        return X_train_text, np.array(Y_train), X_test_text, np.array(Y_test), member_train, member_test, question_train, question_test\n",
    "        \n",
    "    def cut_questions(self, data):\n",
    "        corpus = []\n",
    "        for q in data:\n",
    "            segs = jieba.cut(q, cut_all=False)\n",
    "            final = [seg for seg in segs if seg not in self.stopwords]\n",
    "            corpus.append(' '.join(final))\n",
    "        return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['30383', '56291', '2', '這裡的 letting 加ing是因為also的關係嗎?  You wanna be getting to know a person 是未來進行式吧?並不是因為 also 的關係喔，是因為接續了前面的 wanna be，完整一點應該是： ...and you also wanna be letting that person get to know you.  因為和前面共用一個 wanna be，所以後面省略。  這裡因為沒有 will，所以不算是未來進行式，但現在進行式本身也有未來的意涵喔。', '0'], ['28854', '56291', '2', \"you'll be speaking 為甚麼speak要加ing 還有這句要如何解釋這裡是「未來進行式」的用法，表示某一動作將會、或可能在未來某一時刻進行或持續進行中。  you'll be speaking 就是「你未來、以後都會這樣說」的意思。  這裡用 you'll speak 當然也沒有問題，只是語意上有些許差別而已。\", '0'], ['23656', '78952', '2', '請問My day started out walking ~walking為什麼是現在進行式?另外可否不用start out片語,用My day start to walk with my dog來表示是正確的嗎?這裡是口語上比較簡略的說法，正式書寫應該要加上 with 比較正確，寫作：  My day started out with walking my dog... 表示『用』溜狗開始我的一天。  另外並不能用 My day start to walk with my dog 這樣就變成「我的一天開始去溜狗」，但溜狗的並不是你的一天，而是你本人。  可以試著用 start with（以...開始），像這句就可以寫作： My day started with walking my dog in Central Park...My day started with walking my dog in Central Park... 在這裡，walk會用ing是因為文法句型問題還是因為前面加with?是的，start with 後面要加上一件事，因此原本的動詞要改寫為名詞才能接在後面。', '0']]\n",
      "[['32440', '88024', '2', '為什麼wanna後面要用be getting而不是直接用wanna get to ... ??這裡是在 want to 後面加上現在進行式（be 動詞+現在分詞）的用法，有「一直做、到未來也要做這件事」的口吻。  當然也可以只用 want to get to know，只是口氣上有些微差別而已，但兩種表達方式大致意思是一樣的。', '1'], ['29663', '56291', '2', \" be making friends 這裡是未來進行式嗎?   throughout 這裡昰介係詞嗎您好！  1. 是的，這裡用 going to be making friends 表示現在、未來都會要去交朋友的意思，有動作延續的口吻。當然直接寫成 we're going to make friends 也是完全沒問題的，只是口吻上有一些些差別而已。  2. throughout 在這裡是介係詞沒錯喔，另外也可以參考字典上第 1 條解釋： https://tw.dictionary.yahoo.com/dictionary?p=throughout \", '1'], ['39673', '84855', '2', \"可以寫成這樣嗎？ we're going to make friends, meet people, introduce ourselves to others.可以的！這裡是 be going to 後面加上現在進行式（be動詞+現在分詞）的運用，用 going to be making friends 表示現在、未來都會要去交朋友的意思，有動作延續的口吻。當然直接寫成 we're going to make friends 也是完全沒問題的，只是口吻上有一些些差別而已。\", '1']]\n"
     ]
    }
   ],
   "source": [
    "dh = DataHelper('questions_nondup_dup2.csv')\n",
    "print(dh.unamb_data['2'][:3])\n",
    "print(dh.amb_data['2'][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### get shuffled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape: (3095, 9652)\n",
      "y train shape: (3095,)\n"
     ]
    }
   ],
   "source": [
    "X_train_text, y_train, X_test_text, y_test, member_train, member_test, question_train, question_test = dh.get_shuffled_data()\n",
    "print('X train shape: {}'.format(X_train.shape))\n",
    "print('y train shape: {}'.format(y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## extract features of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class TextFeature(object):\n",
    "    def __init__(self, training_data, testing_data):\n",
    "        self.training_text = training_data\n",
    "        self.testing_text = testing_data\n",
    "        \n",
    "    def get_tfidf(self, use_idf = True):\n",
    "#         texts = self.training_text + self.testing_text\n",
    "        tfidf_vectorizer = TfidfVectorizer(use_idf = True)\n",
    "        tfidf_vectorizer.fit(self.training_text)\n",
    "        X_train = tfidf_vectorizer.transform(self.training_text)\n",
    "        X_test = None\n",
    "        if self.testing_text != None:\n",
    "            X_test = tfidf_vectorizer.transform(self.testing_text)\n",
    "        return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## get features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3095, 9680)\n",
      "(780, 9680)\n"
     ]
    }
   ],
   "source": [
    "tf = TextFeature(X_train_text, X_test_text)\n",
    "X_train, X_test = tf.get_tfidf()\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corss Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all data and get features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: 3875\n",
      "y shape: (3875,)\n"
     ]
    }
   ],
   "source": [
    "X_text, y, member, question_ids = dh.get_all_unambiguous_data()\n",
    "print('X shape: {}'.format(len(X_text)))\n",
    "print('y shape: {}'.format(y.shape))\n",
    "\n",
    "tf = TextFeature(X_text, None)\n",
    "X, x_test = tf.get_tfidf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes corss validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/model_selection/_split.py:581: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "NB_cv = Pipeline([('cls', MultinomialNB()),])\n",
    "parameters = {'cls__alpha': (0.5, 0.8, 1.0, 5, 10)}\n",
    "gs_cls = GridSearchCV(NB_cv, param_grid = parameters, cv = 10, n_jobs = mp.cpu_count()-1)\n",
    "gs_cls = gs_cls.fit(X.todense(), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Paras: {'cls__alpha': 0.5}\n",
      "780\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         2\n",
      "          1       1.00      0.83      0.90       207\n",
      "         10       0.89      0.92      0.91       528\n",
      "         11       0.92      0.19      0.32        57\n",
      "         12       0.91      0.94      0.93       515\n",
      "          2       0.96      0.80      0.87       193\n",
      "          3       0.92      0.85      0.88       448\n",
      "          4       0.00      0.00      0.00        35\n",
      "          5       0.92      0.94      0.93       462\n",
      "          6       1.00      0.09      0.17        96\n",
      "          7       1.00      0.11      0.20        88\n",
      "          8       0.98      0.50      0.66       244\n",
      "          9       0.70      0.99      0.82      1000\n",
      "\n",
      "avg / total       0.86      0.84      0.82      3875\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print('Best Paras:', gs_cls.best_params_)\n",
    "y_predict = gs_cls.predict(X)\n",
    "y_predict_prob = gs_cls.predict_proba(X)\n",
    "\n",
    "infile = 'predicted/NB_question_predict.csv'\n",
    "with open(infile, 'w') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter = ',', quotechar = '\"')\n",
    "    spamwriter.writerow(['question_id', 'member_id', 'question'] + cat)\n",
    "    print(len(question_test))\n",
    "    for i in range(len(question_test)):\n",
    "        writestring = [question_test[i], data_dict[question_test[i]]['member_id'], data_dict[question_test[i]]['question']]\n",
    "        writestring += list(y_predict_prob[i])\n",
    "        spamwriter.writerow(writestring)\n",
    "        \n",
    "print(metrics.classification_report(y, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/model_selection/_split.py:581: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "ename": "JoblibValueError",
     "evalue": "JoblibValueError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/usr/lib/python3.4/runpy.py in _run_module_as_main(mod_name='ipykernel.__main__', alter_argv=1)\n    165         sys.exit(msg)\n    166     main_globals = sys.modules[\"__main__\"].__dict__\n    167     if alter_argv:\n    168         sys.argv[0] = mod_spec.origin\n    169     return _run_code(code, main_globals, None,\n--> 170                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel.__main__', loader=<_f...b/python3.4/dist-packages/ipykernel/__main__.py')\n    171 \n    172 def run_module(mod_name, init_globals=None,\n    173                run_name=None, alter_sys=False):\n    174     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/usr/lib/python3.4/runpy.py in _run_code(code=<code object <module> at 0x7f28cb9af300, file \"/...3.4/dist-packages/ipykernel/__main__.py\", line 1>, run_globals={'__builtins__': <module 'builtins' (built-in)>, '__cached__': '/usr/local/lib/python3.4/dist-packages/ipykernel/__pycache__/__main__.cpython-34.pyc', '__doc__': None, '__file__': '/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py', '__loader__': <_frozen_importlib.SourceFileLoader object>, '__name__': '__main__', '__package__': 'ipykernel', '__spec__': ModuleSpec(name='ipykernel.__main__', loader=<_f...b/python3.4/dist-packages/ipykernel/__main__.py'), 'app': <module 'ipykernel.kernelapp' from '/usr/local/lib/python3.4/dist-packages/ipykernel/kernelapp.py'>}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel.__main__', loader=<_f...b/python3.4/dist-packages/ipykernel/__main__.py'), pkg_name='ipykernel', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x7f28cb9af300, file \"/...3.4/dist-packages/ipykernel/__main__.py\", line 1>\n        run_globals = {'__builtins__': <module 'builtins' (built-in)>, '__cached__': '/usr/local/lib/python3.4/dist-packages/ipykernel/__pycache__/__main__.cpython-34.pyc', '__doc__': None, '__file__': '/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py', '__loader__': <_frozen_importlib.SourceFileLoader object>, '__name__': '__main__', '__package__': 'ipykernel', '__spec__': ModuleSpec(name='ipykernel.__main__', loader=<_f...b/python3.4/dist-packages/ipykernel/__main__.py'), 'app': <module 'ipykernel.kernelapp' from '/usr/local/lib/python3.4/dist-packages/ipykernel/kernelapp.py'>}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py in <module>()\n      1 \n      2 \n----> 3 \n      4 if __name__ == '__main__':\n      5     from ipykernel import kernelapp as app\n      6     app.launch_new_instance()\n      7 \n      8 \n      9 \n     10 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    469             return self.subapp.start()\n    470         if self.poller is not None:\n    471             self.poller.start()\n    472         self.kernel.start()\n    473         try:\n--> 474             ioloop.IOLoop.instance().start()\n    475         except KeyboardInterrupt:\n    476             pass\n    477 \n    478 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/zmq/eventloop/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    172             )\n    173         return loop\n    174     \n    175     def start(self):\n    176         try:\n--> 177             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    178         except ZMQError as e:\n    179             if e.errno == ETERM:\n    180                 # quietly return on ETERM\n    181                 pass\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/tornado/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    882                 self._events.update(event_pairs)\n    883                 while self._events:\n    884                     fd, events = self._events.popitem()\n    885                     try:\n    886                         fd_obj, handler_func = self._handlers[fd]\n--> 887                         handler_func(fd_obj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    888                     except (OSError, IOError) as e:\n    889                         if errno_from_exception(e) == errno.EPIPE:\n    890                             # Happens when the client closes the connection\n    891                             pass\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    271         if self.control_stream:\n    272             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    273 \n    274         def make_dispatcher(stream):\n    275             def dispatcher(msg):\n--> 276                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    277             return dispatcher\n    278 \n    279         for s in self.shell_streams:\n    280             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': \"RF_cv = Pipeline([('cls', RandomForestClassifier...pu_count()-1)\\ngs_cls = gs_cls.fit(X.todense(), y)\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2017, 6, 11, 9, 28, 30, 436727, tzinfo=datetime.timezone.utc), 'msg_id': '57329405E7E44EEC9C38898AC59BB2C4', 'msg_type': 'execute_request', 'session': 'FFA016C43BAA4F83871A6F169C136889', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '57329405E7E44EEC9C38898AC59BB2C4', 'msg_type': 'execute_request', 'parent_header': {}})\n    223             self.log.error(\"UNKNOWN MESSAGE TYPE: %r\", msg_type)\n    224         else:\n    225             self.log.debug(\"%s: %s\", msg_type, msg)\n    226             self.pre_handler_hook()\n    227             try:\n--> 228                 handler(stream, idents, msg)\n        handler = <bound method IPythonKernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'FFA016C43BAA4F83871A6F169C136889']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': \"RF_cv = Pipeline([('cls', RandomForestClassifier...pu_count()-1)\\ngs_cls = gs_cls.fit(X.todense(), y)\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2017, 6, 11, 9, 28, 30, 436727, tzinfo=datetime.timezone.utc), 'msg_id': '57329405E7E44EEC9C38898AC59BB2C4', 'msg_type': 'execute_request', 'session': 'FFA016C43BAA4F83871A6F169C136889', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '57329405E7E44EEC9C38898AC59BB2C4', 'msg_type': 'execute_request', 'parent_header': {}}\n    229             except Exception:\n    230                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    231             finally:\n    232                 self.post_handler_hook()\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'FFA016C43BAA4F83871A6F169C136889'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': \"RF_cv = Pipeline([('cls', RandomForestClassifier...pu_count()-1)\\ngs_cls = gs_cls.fit(X.todense(), y)\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2017, 6, 11, 9, 28, 30, 436727, tzinfo=datetime.timezone.utc), 'msg_id': '57329405E7E44EEC9C38898AC59BB2C4', 'msg_type': 'execute_request', 'session': 'FFA016C43BAA4F83871A6F169C136889', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '57329405E7E44EEC9C38898AC59BB2C4', 'msg_type': 'execute_request', 'parent_header': {}})\n    385         if not silent:\n    386             self.execution_count += 1\n    387             self._publish_execute_input(code, parent, self.execution_count)\n    388 \n    389         reply_content = self.do_execute(code, silent, store_history,\n--> 390                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    391 \n    392         # Flush output before sending the reply.\n    393         sys.stdout.flush()\n    394         sys.stderr.flush()\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code=\"RF_cv = Pipeline([('cls', RandomForestClassifier...pu_count()-1)\\ngs_cls = gs_cls.fit(X.todense(), y)\", silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    191 \n    192         self._forward_input(allow_stdin)\n    193 \n    194         reply_content = {}\n    195         try:\n--> 196             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = \"RF_cv = Pipeline([('cls', RandomForestClassifier...pu_count()-1)\\ngs_cls = gs_cls.fit(X.todense(), y)\"\n        store_history = True\n        silent = False\n    197         finally:\n    198             self._restore_input()\n    199 \n    200         if res.error_before_exec is not None:\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=(\"RF_cv = Pipeline([('cls', RandomForestClassifier...pu_count()-1)\\ngs_cls = gs_cls.fit(X.todense(), y)\",), **kwargs={'silent': False, 'store_history': True})\n    496             )\n    497         self.payload_manager.write_payload(payload)\n    498 \n    499     def run_cell(self, *args, **kwargs):\n    500         self._last_traceback = None\n--> 501         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = (\"RF_cv = Pipeline([('cls', RandomForestClassifier...pu_count()-1)\\ngs_cls = gs_cls.fit(X.todense(), y)\",)\n        kwargs = {'silent': False, 'store_history': True}\n    502 \n    503     def _showtraceback(self, etype, evalue, stb):\n    504         # try to preserve ordering of tracebacks and print statements\n    505         sys.stdout.flush()\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell=\"RF_cv = Pipeline([('cls', RandomForestClassifier...pu_count()-1)\\ngs_cls = gs_cls.fit(X.todense(), y)\", store_history=True, silent=False, shell_futures=True)\n   2712                 self.displayhook.exec_result = result\n   2713 \n   2714                 # Execute the user code\n   2715                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2716                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2717                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2718                 \n   2719                 self.last_execution_succeeded = not has_raised\n   2720 \n   2721                 # Reset this so later displayed values do not modify the\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>], cell_name='<ipython-input-135-b20d01c6b1ef>', interactivity='none', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 7f2886d3df28, executi..._before_exec=None error_in_exec=None result=None>)\n   2816 \n   2817         try:\n   2818             for i, node in enumerate(to_run_exec):\n   2819                 mod = ast.Module([node])\n   2820                 code = compiler(mod, cell_name, \"exec\")\n-> 2821                 if self.run_code(code, result):\n        self.run_code = <bound method ZMQInteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7f2886cd58a0, file \"<ipython-input-135-b20d01c6b1ef>\", line 5>\n        result = <ExecutionResult object at 7f2886d3df28, executi..._before_exec=None error_in_exec=None result=None>\n   2822                     return True\n   2823 \n   2824             for i, node in enumerate(to_run_interactive):\n   2825                 mod = ast.Interactive([node])\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7f2886cd58a0, file \"<ipython-input-135-b20d01c6b1ef>\", line 5>, result=<ExecutionResult object at 7f2886d3df28, executi..._before_exec=None error_in_exec=None result=None>)\n   2876         outflag = 1  # happens in more places, so it's easier as default\n   2877         try:\n   2878             try:\n   2879                 self.hooks.pre_run_code_hook()\n   2880                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2881                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7f2886cd58a0, file \"<ipython-input-135-b20d01c6b1ef>\", line 5>\n        self.user_global_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'DataHelper': <class '__main__.DataHelper'>, 'Grammar': {'0': '其它', '1': '完成式', '10': '連接詞', '11': '假設語氣', '12': '分詞', '13': 'PT', '2': '進行式', '3': '過去式', '4': '未來式', '5': '關係代名詞', ...}, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', \"from collections import defaultdict\\n\\nclass DataH...pus.append(' '.join(final))\\n        return corpus\", \"dh = DataHelper('questions_nondup_dup.csv')\\nprin....unamb_data['2'][:3])\\nprint(dh.amb_data['2'][:3])\", 'import csv\\nimport jieba\\nimport re\\nimport random\\n...Classifier\\nfrom sklearn.svm import LinearSVC, SVC', \"from collections import defaultdict\\n\\nclass DataH...pus.append(' '.join(final))\\n        return corpus\", \"dh = DataHelper('questions_nondup_dup.csv')\\nprin....unamb_data['2'][:3])\\nprint(dh.amb_data['2'][:3])\", \"X_train_text, y_train, X_test_text, y_test, memb...\\nprint('y train shape: {}'.format(y_train.shape))\", 'tf = TextFeature(X_train_text, X_test_text)\\nX_tr..._tfidf()\\nprint(X_train.shape)\\nprint(X_test.shape)', 'class TextFeature(object):\\n    def __init__(self...self.testing_text)\\n        return X_train, X_test', \"with open('questions_nondup_dup.csv') as csvfile...ile):\\n        data_dict[row['question_id']] = row\", 'NB = MultinomialNB(alpha = 1.0)\\nNB.fit(X_train.t...prob[i])\\n        spamwriter.writerow(writestring)', \"X_train_text, y_train, X_test_text, y_test, memb...\\nprint('y train shape: {}'.format(y_train.shape))\", 'tf = TextFeature(X_train_text, X_test_text)\\nX_tr..._tfidf()\\nprint(X_train.shape)\\nprint(X_test.shape)', \"Grammar = {'1': '完成式', '2': '進行式', '3': '過去式', '... '11': '假設語氣', '12': '分詞', '13': 'PT', '0': '其它'}\", \"with open('questions_nondup_dup.csv') as csvfile...ile):\\n        data_dict[row['question_id']] = row\", 'NB = MultinomialNB(alpha = 1.0)\\nNB.fit(X_train.t...prob[i])\\n        spamwriter.writerow(writestring)', \"from collections import defaultdict\\n\\nclass DataH...pus.append(' '.join(final))\\n        return corpus\", \"dh = DataHelper('questions_nondup_dup.csv')\\nprin....unamb_data['2'][:3])\\nprint(dh.amb_data['2'][:3])\", 'class TextFeature(object):\\n    def __init__(self...self.testing_text)\\n        return X_train, X_test', \"X_train_text, y_train, X_test_text, y_test, memb...\\nprint('y train shape: {}'.format(y_train.shape))\", ...], 'LinearSVC': <class 'sklearn.svm.classes.LinearSVC'>, 'MultinomialNB': <class 'sklearn.naive_bayes.MultinomialNB'>, 'NB': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'NB_cv': Pipeline(steps=[('cls', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]), 'Out': {}, ...}\n        self.user_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'DataHelper': <class '__main__.DataHelper'>, 'Grammar': {'0': '其它', '1': '完成式', '10': '連接詞', '11': '假設語氣', '12': '分詞', '13': 'PT', '2': '進行式', '3': '過去式', '4': '未來式', '5': '關係代名詞', ...}, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', \"from collections import defaultdict\\n\\nclass DataH...pus.append(' '.join(final))\\n        return corpus\", \"dh = DataHelper('questions_nondup_dup.csv')\\nprin....unamb_data['2'][:3])\\nprint(dh.amb_data['2'][:3])\", 'import csv\\nimport jieba\\nimport re\\nimport random\\n...Classifier\\nfrom sklearn.svm import LinearSVC, SVC', \"from collections import defaultdict\\n\\nclass DataH...pus.append(' '.join(final))\\n        return corpus\", \"dh = DataHelper('questions_nondup_dup.csv')\\nprin....unamb_data['2'][:3])\\nprint(dh.amb_data['2'][:3])\", \"X_train_text, y_train, X_test_text, y_test, memb...\\nprint('y train shape: {}'.format(y_train.shape))\", 'tf = TextFeature(X_train_text, X_test_text)\\nX_tr..._tfidf()\\nprint(X_train.shape)\\nprint(X_test.shape)', 'class TextFeature(object):\\n    def __init__(self...self.testing_text)\\n        return X_train, X_test', \"with open('questions_nondup_dup.csv') as csvfile...ile):\\n        data_dict[row['question_id']] = row\", 'NB = MultinomialNB(alpha = 1.0)\\nNB.fit(X_train.t...prob[i])\\n        spamwriter.writerow(writestring)', \"X_train_text, y_train, X_test_text, y_test, memb...\\nprint('y train shape: {}'.format(y_train.shape))\", 'tf = TextFeature(X_train_text, X_test_text)\\nX_tr..._tfidf()\\nprint(X_train.shape)\\nprint(X_test.shape)', \"Grammar = {'1': '完成式', '2': '進行式', '3': '過去式', '... '11': '假設語氣', '12': '分詞', '13': 'PT', '0': '其它'}\", \"with open('questions_nondup_dup.csv') as csvfile...ile):\\n        data_dict[row['question_id']] = row\", 'NB = MultinomialNB(alpha = 1.0)\\nNB.fit(X_train.t...prob[i])\\n        spamwriter.writerow(writestring)', \"from collections import defaultdict\\n\\nclass DataH...pus.append(' '.join(final))\\n        return corpus\", \"dh = DataHelper('questions_nondup_dup.csv')\\nprin....unamb_data['2'][:3])\\nprint(dh.amb_data['2'][:3])\", 'class TextFeature(object):\\n    def __init__(self...self.testing_text)\\n        return X_train, X_test', \"X_train_text, y_train, X_test_text, y_test, memb...\\nprint('y train shape: {}'.format(y_train.shape))\", ...], 'LinearSVC': <class 'sklearn.svm.classes.LinearSVC'>, 'MultinomialNB': <class 'sklearn.naive_bayes.MultinomialNB'>, 'NB': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'NB_cv': Pipeline(steps=[('cls', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]), 'Out': {}, ...}\n   2882             finally:\n   2883                 # Reset our crash handler in place\n   2884                 sys.excepthook = old_excepthook\n   2885         except SystemExit as e:\n\n...........................................................................\n/home/pan/Idealab/NTHU/Semester-6/NLP/term_project/<ipython-input-135-b20d01c6b1ef> in <module>()\n      1 \n      2 RF_cv = Pipeline([('cls', RandomForestClassifier()),])\n      3 parameters = {'cls__n_estimator': (10, 20, 64, 128, 256),\n      4               'cls__max_features': ['auto', 'sqrt', 'log2', 'None']}\n----> 5 gs_cls = GridSearchCV(RF_cv, param_grid = parameters, cv = 10, n_jobs = mp.cpu_count()-1)\n      6 gs_cls = gs_cls.fit(X.todense(), y)\n      7 \n      8 \n      9 \n     10 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/model_selection/_search.py in fit(self=GridSearchCV(cv=10, error_score='raise',\n       ...train_score=True,\n       scoring=None, verbose=0), X=matrix([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n   ....],\n        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), y=array(['1', '1', '1', ..., '2', '2', '2'], \n      dtype='<U2'), groups=None)\n    940 \n    941         groups : array-like, with shape (n_samples,), optional\n    942             Group labels for the samples used while splitting the dataset into\n    943             train/test set.\n    944         \"\"\"\n--> 945         return self._fit(X, y, groups, ParameterGrid(self.param_grid))\n        self._fit = <bound method GridSearchCV._fit of GridSearchCV(...rain_score=True,\n       scoring=None, verbose=0)>\n        X = matrix([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n   ....],\n        [ 0.,  0.,  0., ...,  0.,  0.,  0.]])\n        y = array(['1', '1', '1', ..., '2', '2', '2'], \n      dtype='<U2')\n        groups = None\n        self.param_grid = {'cls__max_features': ['auto', 'sqrt', 'log2', 'None'], 'cls__n_estimator': (10, 20, 64, 128, 256)}\n    946 \n    947 \n    948 class RandomizedSearchCV(BaseSearchCV):\n    949     \"\"\"Randomized search on hyper parameters.\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/model_selection/_search.py in _fit(self=GridSearchCV(cv=10, error_score='raise',\n       ...train_score=True,\n       scoring=None, verbose=0), X=matrix([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n   ....],\n        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), y=array(['1', '1', '1', ..., '2', '2', '2'], \n      dtype='<U2'), groups=None, parameter_iterable=<sklearn.model_selection._search.ParameterGrid object>)\n    559                                   fit_params=self.fit_params,\n    560                                   return_train_score=self.return_train_score,\n    561                                   return_n_test_samples=True,\n    562                                   return_times=True, return_parameters=True,\n    563                                   error_score=self.error_score)\n--> 564           for parameters in parameter_iterable\n        parameters = undefined\n        parameter_iterable = <sklearn.model_selection._search.ParameterGrid object>\n    565           for train, test in cv_iter)\n    566 \n    567         # if one choose to see train score, \"out\" will contain train score info\n    568         if self.return_train_score:\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=7), iterable=<generator object <genexpr>>)\n    763             if pre_dispatch == \"all\" or n_jobs == 1:\n    764                 # The iterable was consumed all at once by the above for loop.\n    765                 # No need to wait for async callbacks to trigger to\n    766                 # consumption.\n    767                 self._iterating = False\n--> 768             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=7)>\n    769             # Make sure that we get a last message telling us we are done\n    770             elapsed_time = time.time() - self._start_time\n    771             self._print('Done %3i out of %3i | elapsed: %s finished',\n    772                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nValueError                                         Sun Jun 11 17:28:31 2017\nPID: 5527                                    Python 3.4.3: /usr/bin/python3\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(steps=[('cls', RandomForestClassifier(b...None,\n            verbose=0, warm_start=False))]), matrix([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n   ....],\n        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), array(['1', '1', '1', ..., '2', '2', '2'], \n      dtype='<U2'), <function _passthrough_scorer>, array([  21,   22,   23, ..., 3872, 3873, 3874]), array([   0,    1,    2,    3,    4,    5,    6,... 3694, 3695, 3696, 3697, 3698, 3699, 3700, 3701]), 0, {'cls__max_features': 'auto', 'cls__n_estimator': 10}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(steps=[('cls', RandomForestClassifier(b...None,\n            verbose=0, warm_start=False))]), matrix([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n   ....],\n        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), array(['1', '1', '1', ..., '2', '2', '2'], \n      dtype='<U2'), <function _passthrough_scorer>, array([  21,   22,   23, ..., 3872, 3873, 3874]), array([   0,    1,    2,    3,    4,    5,    6,... 3694, 3695, 3696, 3697, 3698, 3699, 3700, 3701]), 0, {'cls__max_features': 'auto', 'cls__n_estimator': 10})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(steps=[('cls', RandomForestClassifier(b...None,\n            verbose=0, warm_start=False))]), X=matrix([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n   ....],\n        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), y=array(['1', '1', '1', ..., '2', '2', '2'], \n      dtype='<U2'), scorer=<function _passthrough_scorer>, train=array([  21,   22,   23, ..., 3872, 3873, 3874]), test=array([   0,    1,    2,    3,    4,    5,    6,... 3694, 3695, 3696, 3697, 3698, 3699, 3700, 3701]), verbose=0, parameters={'cls__max_features': 'auto', 'cls__n_estimator': 10}, fit_params={}, return_train_score=True, return_parameters=True, return_n_test_samples=True, return_times=True, error_score='raise')\n    222     fit_params = fit_params if fit_params is not None else {}\n    223     fit_params = dict([(k, _index_param_value(X, v, train))\n    224                       for k, v in fit_params.items()])\n    225 \n    226     if parameters is not None:\n--> 227         estimator.set_params(**parameters)\n        estimator.set_params = <bound method Pipeline.set_params of Pipeline(st...one,\n            verbose=0, warm_start=False))])>\n        parameters = {'cls__max_features': 'auto', 'cls__n_estimator': 10}\n    228 \n    229     start_time = time.time()\n    230 \n    231     X_train, y_train = _safe_split(estimator, X, y, train)\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py in set_params(self=Pipeline(steps=[('cls', RandomForestClassifier(b...None,\n            verbose=0, warm_start=False))]), **kwargs={'cls__max_features': 'auto', 'cls__n_estimator': 10})\n    175 \n    176         Returns\n    177         -------\n    178         self\n    179         \"\"\"\n--> 180         self._set_params('steps', **kwargs)\n        self._set_params = <bound method Pipeline._set_params of Pipeline(s...one,\n            verbose=0, warm_start=False))])>\n        kwargs = {'cls__max_features': 'auto', 'cls__n_estimator': 10}\n    181         return self\n    182 \n    183     def _validate_steps(self):\n    184         names, estimators = zip(*self.steps)\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py in _set_params(self=Pipeline(steps=[('cls', RandomForestClassifier(b...None,\n            verbose=0, warm_start=False))]), steps_attr='steps', **params={'cls__max_features': 'auto', 'cls__n_estimator': 10})\n     64         step_names, _ = zip(*getattr(self, steps_attr))\n     65         for name in list(six.iterkeys(params)):\n     66             if '__' not in name and name in step_names:\n     67                 self._replace_step(steps_attr, name, params.pop(name))\n     68         # 3. Step parameters and other initilisation arguments\n---> 69         super(_BasePipeline, self).set_params(**params)\n        self.set_params = <bound method Pipeline.set_params of Pipeline(st...one,\n            verbose=0, warm_start=False))])>\n        params = {'cls__max_features': 'auto', 'cls__n_estimator': 10}\n     70         return self\n     71 \n     72     def _validate_names(self, names):\n     73         if len(set(names)) != len(names):\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/base.py in set_params(self=Pipeline(steps=[('cls', RandomForestClassifier(b...None,\n            verbose=0, warm_start=False))]), **params={'cls__max_features': 'auto', 'cls__n_estimator': 10})\n    279                     raise ValueError('Invalid parameter %s for estimator %s. '\n    280                                      'Check the list of available parameters '\n    281                                      'with `estimator.get_params().keys()`.' %\n    282                                      (name, self))\n    283                 sub_object = valid_params[name]\n--> 284                 sub_object.set_params(**{sub_name: value})\n        sub_object.set_params = <bound method RandomForestClassifier.set_params ...e=None,\n            verbose=0, warm_start=False)>\n        sub_name = 'n_estimator'\n        value = 10\n    285             else:\n    286                 # simple objects case\n    287                 if key not in valid_params:\n    288                     raise ValueError('Invalid parameter %s for estimator %s. '\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/base.py in set_params(self=RandomForestClassifier(bootstrap=True, class_wei...te=None,\n            verbose=0, warm_start=False), **params={'n_estimator': 10})\n    286                 # simple objects case\n    287                 if key not in valid_params:\n    288                     raise ValueError('Invalid parameter %s for estimator %s. '\n    289                                      'Check the list of available parameters '\n    290                                      'with `estimator.get_params().keys()`.' %\n--> 291                                      (key, self.__class__.__name__))\n        key = 'n_estimator'\n        self.__class__.__name__ = 'RandomForestClassifier'\n    292                 setattr(self, key, value)\n    293         return self\n    294 \n    295     def __repr__(self):\n\nValueError: Invalid parameter n_estimator for estimator RandomForestClassifier. Check the list of available parameters with `estimator.get_params().keys()`.\n___________________________________________________________________________",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/_parallel_backends.py\", line 344, in __call__\n    return self.func(*args, **kwargs)\n  File \"/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py\", line 131, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py\", line 131, in <listcomp>\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"/usr/local/lib/python3.4/dist-packages/sklearn/model_selection/_validation.py\", line 227, in _fit_and_score\n    estimator.set_params(**parameters)\n  File \"/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py\", line 180, in set_params\n    self._set_params('steps', **kwargs)\n  File \"/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py\", line 69, in _set_params\n    super(_BasePipeline, self).set_params(**params)\n  File \"/usr/local/lib/python3.4/dist-packages/sklearn/base.py\", line 284, in set_params\n    sub_object.set_params(**{sub_name: value})\n  File \"/usr/local/lib/python3.4/dist-packages/sklearn/base.py\", line 291, in set_params\n    (key, self.__class__.__name__))\nValueError: Invalid parameter n_estimator for estimator RandomForestClassifier. Check the list of available parameters with `estimator.get_params().keys()`.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.4/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/_parallel_backends.py\", line 353, in __call__\n    raise TransportableException(text, e_type)\nsklearn.externals.joblib.my_exceptions.TransportableException: TransportableException\n___________________________________________________________________________\nValueError                                         Sun Jun 11 17:28:31 2017\nPID: 5527                                    Python 3.4.3: /usr/bin/python3\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(steps=[('cls', RandomForestClassifier(b...None,\n            verbose=0, warm_start=False))]), matrix([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n   ....],\n        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), array(['1', '1', '1', ..., '2', '2', '2'], \n      dtype='<U2'), <function _passthrough_scorer>, array([  21,   22,   23, ..., 3872, 3873, 3874]), array([   0,    1,    2,    3,    4,    5,    6,... 3694, 3695, 3696, 3697, 3698, 3699, 3700, 3701]), 0, {'cls__max_features': 'auto', 'cls__n_estimator': 10}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(steps=[('cls', RandomForestClassifier(b...None,\n            verbose=0, warm_start=False))]), matrix([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n   ....],\n        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), array(['1', '1', '1', ..., '2', '2', '2'], \n      dtype='<U2'), <function _passthrough_scorer>, array([  21,   22,   23, ..., 3872, 3873, 3874]), array([   0,    1,    2,    3,    4,    5,    6,... 3694, 3695, 3696, 3697, 3698, 3699, 3700, 3701]), 0, {'cls__max_features': 'auto', 'cls__n_estimator': 10})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(steps=[('cls', RandomForestClassifier(b...None,\n            verbose=0, warm_start=False))]), X=matrix([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n   ....],\n        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), y=array(['1', '1', '1', ..., '2', '2', '2'], \n      dtype='<U2'), scorer=<function _passthrough_scorer>, train=array([  21,   22,   23, ..., 3872, 3873, 3874]), test=array([   0,    1,    2,    3,    4,    5,    6,... 3694, 3695, 3696, 3697, 3698, 3699, 3700, 3701]), verbose=0, parameters={'cls__max_features': 'auto', 'cls__n_estimator': 10}, fit_params={}, return_train_score=True, return_parameters=True, return_n_test_samples=True, return_times=True, error_score='raise')\n    222     fit_params = fit_params if fit_params is not None else {}\n    223     fit_params = dict([(k, _index_param_value(X, v, train))\n    224                       for k, v in fit_params.items()])\n    225 \n    226     if parameters is not None:\n--> 227         estimator.set_params(**parameters)\n        estimator.set_params = <bound method Pipeline.set_params of Pipeline(st...one,\n            verbose=0, warm_start=False))])>\n        parameters = {'cls__max_features': 'auto', 'cls__n_estimator': 10}\n    228 \n    229     start_time = time.time()\n    230 \n    231     X_train, y_train = _safe_split(estimator, X, y, train)\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py in set_params(self=Pipeline(steps=[('cls', RandomForestClassifier(b...None,\n            verbose=0, warm_start=False))]), **kwargs={'cls__max_features': 'auto', 'cls__n_estimator': 10})\n    175 \n    176         Returns\n    177         -------\n    178         self\n    179         \"\"\"\n--> 180         self._set_params('steps', **kwargs)\n        self._set_params = <bound method Pipeline._set_params of Pipeline(s...one,\n            verbose=0, warm_start=False))])>\n        kwargs = {'cls__max_features': 'auto', 'cls__n_estimator': 10}\n    181         return self\n    182 \n    183     def _validate_steps(self):\n    184         names, estimators = zip(*self.steps)\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py in _set_params(self=Pipeline(steps=[('cls', RandomForestClassifier(b...None,\n            verbose=0, warm_start=False))]), steps_attr='steps', **params={'cls__max_features': 'auto', 'cls__n_estimator': 10})\n     64         step_names, _ = zip(*getattr(self, steps_attr))\n     65         for name in list(six.iterkeys(params)):\n     66             if '__' not in name and name in step_names:\n     67                 self._replace_step(steps_attr, name, params.pop(name))\n     68         # 3. Step parameters and other initilisation arguments\n---> 69         super(_BasePipeline, self).set_params(**params)\n        self.set_params = <bound method Pipeline.set_params of Pipeline(st...one,\n            verbose=0, warm_start=False))])>\n        params = {'cls__max_features': 'auto', 'cls__n_estimator': 10}\n     70         return self\n     71 \n     72     def _validate_names(self, names):\n     73         if len(set(names)) != len(names):\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/base.py in set_params(self=Pipeline(steps=[('cls', RandomForestClassifier(b...None,\n            verbose=0, warm_start=False))]), **params={'cls__max_features': 'auto', 'cls__n_estimator': 10})\n    279                     raise ValueError('Invalid parameter %s for estimator %s. '\n    280                                      'Check the list of available parameters '\n    281                                      'with `estimator.get_params().keys()`.' %\n    282                                      (name, self))\n    283                 sub_object = valid_params[name]\n--> 284                 sub_object.set_params(**{sub_name: value})\n        sub_object.set_params = <bound method RandomForestClassifier.set_params ...e=None,\n            verbose=0, warm_start=False)>\n        sub_name = 'n_estimator'\n        value = 10\n    285             else:\n    286                 # simple objects case\n    287                 if key not in valid_params:\n    288                     raise ValueError('Invalid parameter %s for estimator %s. '\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/base.py in set_params(self=RandomForestClassifier(bootstrap=True, class_wei...te=None,\n            verbose=0, warm_start=False), **params={'n_estimator': 10})\n    286                 # simple objects case\n    287                 if key not in valid_params:\n    288                     raise ValueError('Invalid parameter %s for estimator %s. '\n    289                                      'Check the list of available parameters '\n    290                                      'with `estimator.get_params().keys()`.' %\n--> 291                                      (key, self.__class__.__name__))\n        key = 'n_estimator'\n        self.__class__.__name__ = 'RandomForestClassifier'\n    292                 setattr(self, key, value)\n    293         return self\n    294 \n    295     def __repr__(self):\n\nValueError: Invalid parameter n_estimator for estimator RandomForestClassifier. Check the list of available parameters with `estimator.get_params().keys()`.\n___________________________________________________________________________\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTransportableException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    681\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m'timeout'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTransportableException\u001b[0m: TransportableException\n___________________________________________________________________________\nValueError                                         Sun Jun 11 17:28:31 2017\nPID: 5527                                    Python 3.4.3: /usr/bin/python3\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(steps=[('cls', RandomForestClassifier(b...None,\n            verbose=0, warm_start=False))]), matrix([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n   ....],\n        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), array(['1', '1', '1', ..., '2', '2', '2'], \n      dtype='<U2'), <function _passthrough_scorer>, array([  21,   22,   23, ..., 3872, 3873, 3874]), array([   0,    1,    2,    3,    4,    5,    6,... 3694, 3695, 3696, 3697, 3698, 3699, 3700, 3701]), 0, {'cls__max_features': 'auto', 'cls__n_estimator': 10}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(steps=[('cls', RandomForestClassifier(b...None,\n            verbose=0, warm_start=False))]), matrix([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n   ....],\n        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), array(['1', '1', '1', ..., '2', '2', '2'], \n      dtype='<U2'), <function _passthrough_scorer>, array([  21,   22,   23, ..., 3872, 3873, 3874]), array([   0,    1,    2,    3,    4,    5,    6,... 3694, 3695, 3696, 3697, 3698, 3699, 3700, 3701]), 0, {'cls__max_features': 'auto', 'cls__n_estimator': 10})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(steps=[('cls', RandomForestClassifier(b...None,\n            verbose=0, warm_start=False))]), X=matrix([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n   ....],\n        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), y=array(['1', '1', '1', ..., '2', '2', '2'], \n      dtype='<U2'), scorer=<function _passthrough_scorer>, train=array([  21,   22,   23, ..., 3872, 3873, 3874]), test=array([   0,    1,    2,    3,    4,    5,    6,... 3694, 3695, 3696, 3697, 3698, 3699, 3700, 3701]), verbose=0, parameters={'cls__max_features': 'auto', 'cls__n_estimator': 10}, fit_params={}, return_train_score=True, return_parameters=True, return_n_test_samples=True, return_times=True, error_score='raise')\n    222     fit_params = fit_params if fit_params is not None else {}\n    223     fit_params = dict([(k, _index_param_value(X, v, train))\n    224                       for k, v in fit_params.items()])\n    225 \n    226     if parameters is not None:\n--> 227         estimator.set_params(**parameters)\n        estimator.set_params = <bound method Pipeline.set_params of Pipeline(st...one,\n            verbose=0, warm_start=False))])>\n        parameters = {'cls__max_features': 'auto', 'cls__n_estimator': 10}\n    228 \n    229     start_time = time.time()\n    230 \n    231     X_train, y_train = _safe_split(estimator, X, y, train)\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py in set_params(self=Pipeline(steps=[('cls', RandomForestClassifier(b...None,\n            verbose=0, warm_start=False))]), **kwargs={'cls__max_features': 'auto', 'cls__n_estimator': 10})\n    175 \n    176         Returns\n    177         -------\n    178         self\n    179         \"\"\"\n--> 180         self._set_params('steps', **kwargs)\n        self._set_params = <bound method Pipeline._set_params of Pipeline(s...one,\n            verbose=0, warm_start=False))])>\n        kwargs = {'cls__max_features': 'auto', 'cls__n_estimator': 10}\n    181         return self\n    182 \n    183     def _validate_steps(self):\n    184         names, estimators = zip(*self.steps)\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py in _set_params(self=Pipeline(steps=[('cls', RandomForestClassifier(b...None,\n            verbose=0, warm_start=False))]), steps_attr='steps', **params={'cls__max_features': 'auto', 'cls__n_estimator': 10})\n     64         step_names, _ = zip(*getattr(self, steps_attr))\n     65         for name in list(six.iterkeys(params)):\n     66             if '__' not in name and name in step_names:\n     67                 self._replace_step(steps_attr, name, params.pop(name))\n     68         # 3. Step parameters and other initilisation arguments\n---> 69         super(_BasePipeline, self).set_params(**params)\n        self.set_params = <bound method Pipeline.set_params of Pipeline(st...one,\n            verbose=0, warm_start=False))])>\n        params = {'cls__max_features': 'auto', 'cls__n_estimator': 10}\n     70         return self\n     71 \n     72     def _validate_names(self, names):\n     73         if len(set(names)) != len(names):\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/base.py in set_params(self=Pipeline(steps=[('cls', RandomForestClassifier(b...None,\n            verbose=0, warm_start=False))]), **params={'cls__max_features': 'auto', 'cls__n_estimator': 10})\n    279                     raise ValueError('Invalid parameter %s for estimator %s. '\n    280                                      'Check the list of available parameters '\n    281                                      'with `estimator.get_params().keys()`.' %\n    282                                      (name, self))\n    283                 sub_object = valid_params[name]\n--> 284                 sub_object.set_params(**{sub_name: value})\n        sub_object.set_params = <bound method RandomForestClassifier.set_params ...e=None,\n            verbose=0, warm_start=False)>\n        sub_name = 'n_estimator'\n        value = 10\n    285             else:\n    286                 # simple objects case\n    287                 if key not in valid_params:\n    288                     raise ValueError('Invalid parameter %s for estimator %s. '\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/base.py in set_params(self=RandomForestClassifier(bootstrap=True, class_wei...te=None,\n            verbose=0, warm_start=False), **params={'n_estimator': 10})\n    286                 # simple objects case\n    287                 if key not in valid_params:\n    288                     raise ValueError('Invalid parameter %s for estimator %s. '\n    289                                      'Check the list of available parameters '\n    290                                      'with `estimator.get_params().keys()`.' %\n--> 291                                      (key, self.__class__.__name__))\n        key = 'n_estimator'\n        self.__class__.__name__ = 'RandomForestClassifier'\n    292                 setattr(self, key, value)\n    293         return self\n    294 \n    295     def __repr__(self):\n\nValueError: Invalid parameter n_estimator for estimator RandomForestClassifier. Check the list of available parameters with `estimator.get_params().keys()`.\n___________________________________________________________________________",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mJoblibValueError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-b20d01c6b1ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m               'cls__max_features': ['auto', 'sqrt', 'log2', 'None']}\n\u001b[1;32m      4\u001b[0m \u001b[0mgs_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRF_cv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mgs_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    943\u001b[0m             \u001b[0mtrain\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtest\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \"\"\"\n\u001b[0;32m--> 945\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, groups, parameter_iterable)\u001b[0m\n\u001b[1;32m    562\u001b[0m                                   \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m--> 564\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m           for train, test in cv_iter)\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    766\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJoblibValueError\u001b[0m: JoblibValueError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/usr/lib/python3.4/runpy.py in _run_module_as_main(mod_name='ipykernel.__main__', alter_argv=1)\n    165         sys.exit(msg)\n    166     main_globals = sys.modules[\"__main__\"].__dict__\n    167     if alter_argv:\n    168         sys.argv[0] = mod_spec.origin\n    169     return _run_code(code, main_globals, None,\n--> 170                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel.__main__', loader=<_f...b/python3.4/dist-packages/ipykernel/__main__.py')\n    171 \n    172 def run_module(mod_name, init_globals=None,\n    173                run_name=None, alter_sys=False):\n    174     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/usr/lib/python3.4/runpy.py in _run_code(code=<code object <module> at 0x7f28cb9af300, file \"/...3.4/dist-packages/ipykernel/__main__.py\", line 1>, run_globals={'__builtins__': <module 'builtins' (built-in)>, '__cached__': '/usr/local/lib/python3.4/dist-packages/ipykernel/__pycache__/__main__.cpython-34.pyc', '__doc__': None, '__file__': '/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py', '__loader__': <_frozen_importlib.SourceFileLoader object>, '__name__': '__main__', '__package__': 'ipykernel', '__spec__': ModuleSpec(name='ipykernel.__main__', loader=<_f...b/python3.4/dist-packages/ipykernel/__main__.py'), 'app': <module 'ipykernel.kernelapp' from '/usr/local/lib/python3.4/dist-packages/ipykernel/kernelapp.py'>}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel.__main__', loader=<_f...b/python3.4/dist-packages/ipykernel/__main__.py'), pkg_name='ipykernel', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x7f28cb9af300, file \"/...3.4/dist-packages/ipykernel/__main__.py\", line 1>\n        run_globals = {'__builtins__': <module 'builtins' (built-in)>, '__cached__': '/usr/local/lib/python3.4/dist-packages/ipykernel/__pycache__/__main__.cpython-34.pyc', '__doc__': None, '__file__': '/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py', '__loader__': <_frozen_importlib.SourceFileLoader object>, '__name__': '__main__', '__package__': 'ipykernel', '__spec__': ModuleSpec(name='ipykernel.__main__', loader=<_f...b/python3.4/dist-packages/ipykernel/__main__.py'), 'app': <module 'ipykernel.kernelapp' from '/usr/local/lib/python3.4/dist-packages/ipykernel/kernelapp.py'>}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py in <module>()\n      1 \n      2 \n----> 3 \n      4 if __name__ == '__main__':\n      5     from ipykernel import kernelapp as app\n      6     app.launch_new_instance()\n      7 \n      8 \n      9 \n     10 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    469             return self.subapp.start()\n    470         if self.poller is not None:\n    471             self.poller.start()\n    472         self.kernel.start()\n    473         try:\n--> 474             ioloop.IOLoop.instance().start()\n    475         except KeyboardInterrupt:\n    476             pass\n    477 \n    478 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/zmq/eventloop/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    172             )\n    173         return loop\n    174     \n    175     def start(self):\n    176         try:\n--> 177             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    178         except ZMQError as e:\n    179             if e.errno == ETERM:\n    180                 # quietly return on ETERM\n    181                 pass\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/tornado/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    882                 self._events.update(event_pairs)\n    883                 while self._events:\n    884                     fd, events = self._events.popitem()\n    885                     try:\n    886                         fd_obj, handler_func = self._handlers[fd]\n--> 887                         handler_func(fd_obj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    888                     except (OSError, IOError) as e:\n    889                         if errno_from_exception(e) == errno.EPIPE:\n    890                             # Happens when the client closes the connection\n    891                             pass\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    271         if self.control_stream:\n    272             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    273 \n    274         def make_dispatcher(stream):\n    275             def dispatcher(msg):\n--> 276                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    277             return dispatcher\n    278 \n    279         for s in self.shell_streams:\n    280             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': \"RF_cv = Pipeline([('cls', RandomForestClassifier...pu_count()-1)\\ngs_cls = gs_cls.fit(X.todense(), y)\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2017, 6, 11, 9, 28, 30, 436727, tzinfo=datetime.timezone.utc), 'msg_id': '57329405E7E44EEC9C38898AC59BB2C4', 'msg_type': 'execute_request', 'session': 'FFA016C43BAA4F83871A6F169C136889', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '57329405E7E44EEC9C38898AC59BB2C4', 'msg_type': 'execute_request', 'parent_header': {}})\n    223             self.log.error(\"UNKNOWN MESSAGE TYPE: %r\", msg_type)\n    224         else:\n    225             self.log.debug(\"%s: %s\", msg_type, msg)\n    226             self.pre_handler_hook()\n    227             try:\n--> 228                 handler(stream, idents, msg)\n        handler = <bound method IPythonKernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'FFA016C43BAA4F83871A6F169C136889']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': \"RF_cv = Pipeline([('cls', RandomForestClassifier...pu_count()-1)\\ngs_cls = gs_cls.fit(X.todense(), y)\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2017, 6, 11, 9, 28, 30, 436727, tzinfo=datetime.timezone.utc), 'msg_id': '57329405E7E44EEC9C38898AC59BB2C4', 'msg_type': 'execute_request', 'session': 'FFA016C43BAA4F83871A6F169C136889', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '57329405E7E44EEC9C38898AC59BB2C4', 'msg_type': 'execute_request', 'parent_header': {}}\n    229             except Exception:\n    230                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    231             finally:\n    232                 self.post_handler_hook()\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'FFA016C43BAA4F83871A6F169C136889'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': \"RF_cv = Pipeline([('cls', RandomForestClassifier...pu_count()-1)\\ngs_cls = gs_cls.fit(X.todense(), y)\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2017, 6, 11, 9, 28, 30, 436727, tzinfo=datetime.timezone.utc), 'msg_id': '57329405E7E44EEC9C38898AC59BB2C4', 'msg_type': 'execute_request', 'session': 'FFA016C43BAA4F83871A6F169C136889', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '57329405E7E44EEC9C38898AC59BB2C4', 'msg_type': 'execute_request', 'parent_header': {}})\n    385         if not silent:\n    386             self.execution_count += 1\n    387             self._publish_execute_input(code, parent, self.execution_count)\n    388 \n    389         reply_content = self.do_execute(code, silent, store_history,\n--> 390                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    391 \n    392         # Flush output before sending the reply.\n    393         sys.stdout.flush()\n    394         sys.stderr.flush()\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code=\"RF_cv = Pipeline([('cls', RandomForestClassifier...pu_count()-1)\\ngs_cls = gs_cls.fit(X.todense(), y)\", silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    191 \n    192         self._forward_input(allow_stdin)\n    193 \n    194         reply_content = {}\n    195         try:\n--> 196             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = \"RF_cv = Pipeline([('cls', RandomForestClassifier...pu_count()-1)\\ngs_cls = gs_cls.fit(X.todense(), y)\"\n        store_history = True\n        silent = False\n    197         finally:\n    198             self._restore_input()\n    199 \n    200         if res.error_before_exec is not None:\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=(\"RF_cv = Pipeline([('cls', RandomForestClassifier...pu_count()-1)\\ngs_cls = gs_cls.fit(X.todense(), y)\",), **kwargs={'silent': False, 'store_history': True})\n    496             )\n    497         self.payload_manager.write_payload(payload)\n    498 \n    499     def run_cell(self, *args, **kwargs):\n    500         self._last_traceback = None\n--> 501         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = (\"RF_cv = Pipeline([('cls', RandomForestClassifier...pu_count()-1)\\ngs_cls = gs_cls.fit(X.todense(), y)\",)\n        kwargs = {'silent': False, 'store_history': True}\n    502 \n    503     def _showtraceback(self, etype, evalue, stb):\n    504         # try to preserve ordering of tracebacks and print statements\n    505         sys.stdout.flush()\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell=\"RF_cv = Pipeline([('cls', RandomForestClassifier...pu_count()-1)\\ngs_cls = gs_cls.fit(X.todense(), y)\", store_history=True, silent=False, shell_futures=True)\n   2712                 self.displayhook.exec_result = result\n   2713 \n   2714                 # Execute the user code\n   2715                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2716                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2717                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2718                 \n   2719                 self.last_execution_succeeded = not has_raised\n   2720 \n   2721                 # Reset this so later displayed values do not modify the\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>], cell_name='<ipython-input-135-b20d01c6b1ef>', interactivity='none', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 7f2886d3df28, executi..._before_exec=None error_in_exec=None result=None>)\n   2816 \n   2817         try:\n   2818             for i, node in enumerate(to_run_exec):\n   2819                 mod = ast.Module([node])\n   2820                 code = compiler(mod, cell_name, \"exec\")\n-> 2821                 if self.run_code(code, result):\n        self.run_code = <bound method ZMQInteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7f2886cd58a0, file \"<ipython-input-135-b20d01c6b1ef>\", line 5>\n        result = <ExecutionResult object at 7f2886d3df28, executi..._before_exec=None error_in_exec=None result=None>\n   2822                     return True\n   2823 \n   2824             for i, node in enumerate(to_run_interactive):\n   2825                 mod = ast.Interactive([node])\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7f2886cd58a0, file \"<ipython-input-135-b20d01c6b1ef>\", line 5>, result=<ExecutionResult object at 7f2886d3df28, executi..._before_exec=None error_in_exec=None result=None>)\n   2876         outflag = 1  # happens in more places, so it's easier as default\n   2877         try:\n   2878             try:\n   2879                 self.hooks.pre_run_code_hook()\n   2880                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2881                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7f2886cd58a0, file \"<ipython-input-135-b20d01c6b1ef>\", line 5>\n        self.user_global_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'DataHelper': <class '__main__.DataHelper'>, 'Grammar': {'0': '其它', '1': '完成式', '10': '連接詞', '11': '假設語氣', '12': '分詞', '13': 'PT', '2': '進行式', '3': '過去式', '4': '未來式', '5': '關係代名詞', ...}, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', \"from collections import defaultdict\\n\\nclass DataH...pus.append(' '.join(final))\\n        return corpus\", \"dh = DataHelper('questions_nondup_dup.csv')\\nprin....unamb_data['2'][:3])\\nprint(dh.amb_data['2'][:3])\", 'import csv\\nimport jieba\\nimport re\\nimport random\\n...Classifier\\nfrom sklearn.svm import LinearSVC, SVC', \"from collections import defaultdict\\n\\nclass DataH...pus.append(' '.join(final))\\n        return corpus\", \"dh = DataHelper('questions_nondup_dup.csv')\\nprin....unamb_data['2'][:3])\\nprint(dh.amb_data['2'][:3])\", \"X_train_text, y_train, X_test_text, y_test, memb...\\nprint('y train shape: {}'.format(y_train.shape))\", 'tf = TextFeature(X_train_text, X_test_text)\\nX_tr..._tfidf()\\nprint(X_train.shape)\\nprint(X_test.shape)', 'class TextFeature(object):\\n    def __init__(self...self.testing_text)\\n        return X_train, X_test', \"with open('questions_nondup_dup.csv') as csvfile...ile):\\n        data_dict[row['question_id']] = row\", 'NB = MultinomialNB(alpha = 1.0)\\nNB.fit(X_train.t...prob[i])\\n        spamwriter.writerow(writestring)', \"X_train_text, y_train, X_test_text, y_test, memb...\\nprint('y train shape: {}'.format(y_train.shape))\", 'tf = TextFeature(X_train_text, X_test_text)\\nX_tr..._tfidf()\\nprint(X_train.shape)\\nprint(X_test.shape)', \"Grammar = {'1': '完成式', '2': '進行式', '3': '過去式', '... '11': '假設語氣', '12': '分詞', '13': 'PT', '0': '其它'}\", \"with open('questions_nondup_dup.csv') as csvfile...ile):\\n        data_dict[row['question_id']] = row\", 'NB = MultinomialNB(alpha = 1.0)\\nNB.fit(X_train.t...prob[i])\\n        spamwriter.writerow(writestring)', \"from collections import defaultdict\\n\\nclass DataH...pus.append(' '.join(final))\\n        return corpus\", \"dh = DataHelper('questions_nondup_dup.csv')\\nprin....unamb_data['2'][:3])\\nprint(dh.amb_data['2'][:3])\", 'class TextFeature(object):\\n    def __init__(self...self.testing_text)\\n        return X_train, X_test', \"X_train_text, y_train, X_test_text, y_test, memb...\\nprint('y train shape: {}'.format(y_train.shape))\", ...], 'LinearSVC': <class 'sklearn.svm.classes.LinearSVC'>, 'MultinomialNB': <class 'sklearn.naive_bayes.MultinomialNB'>, 'NB': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'NB_cv': Pipeline(steps=[('cls', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]), 'Out': {}, ...}\n        self.user_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'DataHelper': <class '__main__.DataHelper'>, 'Grammar': {'0': '其它', '1': '完成式', '10': '連接詞', '11': '假設語氣', '12': '分詞', '13': 'PT', '2': '進行式', '3': '過去式', '4': '未來式', '5': '關係代名詞', ...}, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', \"from collections import defaultdict\\n\\nclass DataH...pus.append(' '.join(final))\\n        return corpus\", \"dh = DataHelper('questions_nondup_dup.csv')\\nprin....unamb_data['2'][:3])\\nprint(dh.amb_data['2'][:3])\", 'import csv\\nimport jieba\\nimport re\\nimport random\\n...Classifier\\nfrom sklearn.svm import LinearSVC, SVC', \"from collections import defaultdict\\n\\nclass DataH...pus.append(' '.join(final))\\n        return corpus\", \"dh = DataHelper('questions_nondup_dup.csv')\\nprin....unamb_data['2'][:3])\\nprint(dh.amb_data['2'][:3])\", \"X_train_text, y_train, X_test_text, y_test, memb...\\nprint('y train shape: {}'.format(y_train.shape))\", 'tf = TextFeature(X_train_text, X_test_text)\\nX_tr..._tfidf()\\nprint(X_train.shape)\\nprint(X_test.shape)', 'class TextFeature(object):\\n    def __init__(self...self.testing_text)\\n        return X_train, X_test', \"with open('questions_nondup_dup.csv') as csvfile...ile):\\n        data_dict[row['question_id']] = row\", 'NB = MultinomialNB(alpha = 1.0)\\nNB.fit(X_train.t...prob[i])\\n        spamwriter.writerow(writestring)', \"X_train_text, y_train, X_test_text, y_test, memb...\\nprint('y train shape: {}'.format(y_train.shape))\", 'tf = TextFeature(X_train_text, X_test_text)\\nX_tr..._tfidf()\\nprint(X_train.shape)\\nprint(X_test.shape)', \"Grammar = {'1': '完成式', '2': '進行式', '3': '過去式', '... '11': '假設語氣', '12': '分詞', '13': 'PT', '0': '其它'}\", \"with open('questions_nondup_dup.csv') as csvfile...ile):\\n        data_dict[row['question_id']] = row\", 'NB = MultinomialNB(alpha = 1.0)\\nNB.fit(X_train.t...prob[i])\\n        spamwriter.writerow(writestring)', \"from collections import defaultdict\\n\\nclass DataH...pus.append(' '.join(final))\\n        return corpus\", \"dh = DataHelper('questions_nondup_dup.csv')\\nprin....unamb_data['2'][:3])\\nprint(dh.amb_data['2'][:3])\", 'class TextFeature(object):\\n    def __init__(self...self.testing_text)\\n        return X_train, X_test', \"X_train_text, y_train, X_test_text, y_test, memb...\\nprint('y train shape: {}'.format(y_train.shape))\", ...], 'LinearSVC': <class 'sklearn.svm.classes.LinearSVC'>, 'MultinomialNB': <class 'sklearn.naive_bayes.MultinomialNB'>, 'NB': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 'NB_cv': Pipeline(steps=[('cls', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]), 'Out': {}, ...}\n   2882             finally:\n   2883                 # Reset our crash handler in place\n   2884                 sys.excepthook = old_excepthook\n   2885         except SystemExit as e:\n\n...........................................................................\n/home/pan/Idealab/NTHU/Semester-6/NLP/term_project/<ipython-input-135-b20d01c6b1ef> in <module>()\n      1 \n      2 RF_cv = Pipeline([('cls', RandomForestClassifier()),])\n      3 parameters = {'cls__n_estimator': (10, 20, 64, 128, 256),\n      4               'cls__max_features': ['auto', 'sqrt', 'log2', 'None']}\n----> 5 gs_cls = GridSearchCV(RF_cv, param_grid = parameters, cv = 10, n_jobs = mp.cpu_count()-1)\n      6 gs_cls = gs_cls.fit(X.todense(), y)\n      7 \n      8 \n      9 \n     10 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/model_selection/_search.py in fit(self=GridSearchCV(cv=10, error_score='raise',\n       ...train_score=True,\n       scoring=None, verbose=0), X=matrix([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n   ....],\n        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), y=array(['1', '1', '1', ..., '2', '2', '2'], \n      dtype='<U2'), groups=None)\n    940 \n    941         groups : array-like, with shape (n_samples,), optional\n    942             Group labels for the samples used while splitting the dataset into\n    943             train/test set.\n    944         \"\"\"\n--> 945         return self._fit(X, y, groups, ParameterGrid(self.param_grid))\n        self._fit = <bound method GridSearchCV._fit of GridSearchCV(...rain_score=True,\n       scoring=None, verbose=0)>\n        X = matrix([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n   ....],\n        [ 0.,  0.,  0., ...,  0.,  0.,  0.]])\n        y = array(['1', '1', '1', ..., '2', '2', '2'], \n      dtype='<U2')\n        groups = None\n        self.param_grid = {'cls__max_features': ['auto', 'sqrt', 'log2', 'None'], 'cls__n_estimator': (10, 20, 64, 128, 256)}\n    946 \n    947 \n    948 class RandomizedSearchCV(BaseSearchCV):\n    949     \"\"\"Randomized search on hyper parameters.\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/model_selection/_search.py in _fit(self=GridSearchCV(cv=10, error_score='raise',\n       ...train_score=True,\n       scoring=None, verbose=0), X=matrix([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n   ....],\n        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), y=array(['1', '1', '1', ..., '2', '2', '2'], \n      dtype='<U2'), groups=None, parameter_iterable=<sklearn.model_selection._search.ParameterGrid object>)\n    559                                   fit_params=self.fit_params,\n    560                                   return_train_score=self.return_train_score,\n    561                                   return_n_test_samples=True,\n    562                                   return_times=True, return_parameters=True,\n    563                                   error_score=self.error_score)\n--> 564           for parameters in parameter_iterable\n        parameters = undefined\n        parameter_iterable = <sklearn.model_selection._search.ParameterGrid object>\n    565           for train, test in cv_iter)\n    566 \n    567         # if one choose to see train score, \"out\" will contain train score info\n    568         if self.return_train_score:\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=7), iterable=<generator object <genexpr>>)\n    763             if pre_dispatch == \"all\" or n_jobs == 1:\n    764                 # The iterable was consumed all at once by the above for loop.\n    765                 # No need to wait for async callbacks to trigger to\n    766                 # consumption.\n    767                 self._iterating = False\n--> 768             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=7)>\n    769             # Make sure that we get a last message telling us we are done\n    770             elapsed_time = time.time() - self._start_time\n    771             self._print('Done %3i out of %3i | elapsed: %s finished',\n    772                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nValueError                                         Sun Jun 11 17:28:31 2017\nPID: 5527                                    Python 3.4.3: /usr/bin/python3\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(steps=[('cls', RandomForestClassifier(b...None,\n            verbose=0, warm_start=False))]), matrix([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n   ....],\n        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), array(['1', '1', '1', ..., '2', '2', '2'], \n      dtype='<U2'), <function _passthrough_scorer>, array([  21,   22,   23, ..., 3872, 3873, 3874]), array([   0,    1,    2,    3,    4,    5,    6,... 3694, 3695, 3696, 3697, 3698, 3699, 3700, 3701]), 0, {'cls__max_features': 'auto', 'cls__n_estimator': 10}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(steps=[('cls', RandomForestClassifier(b...None,\n            verbose=0, warm_start=False))]), matrix([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n   ....],\n        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), array(['1', '1', '1', ..., '2', '2', '2'], \n      dtype='<U2'), <function _passthrough_scorer>, array([  21,   22,   23, ..., 3872, 3873, 3874]), array([   0,    1,    2,    3,    4,    5,    6,... 3694, 3695, 3696, 3697, 3698, 3699, 3700, 3701]), 0, {'cls__max_features': 'auto', 'cls__n_estimator': 10})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(steps=[('cls', RandomForestClassifier(b...None,\n            verbose=0, warm_start=False))]), X=matrix([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n   ....],\n        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), y=array(['1', '1', '1', ..., '2', '2', '2'], \n      dtype='<U2'), scorer=<function _passthrough_scorer>, train=array([  21,   22,   23, ..., 3872, 3873, 3874]), test=array([   0,    1,    2,    3,    4,    5,    6,... 3694, 3695, 3696, 3697, 3698, 3699, 3700, 3701]), verbose=0, parameters={'cls__max_features': 'auto', 'cls__n_estimator': 10}, fit_params={}, return_train_score=True, return_parameters=True, return_n_test_samples=True, return_times=True, error_score='raise')\n    222     fit_params = fit_params if fit_params is not None else {}\n    223     fit_params = dict([(k, _index_param_value(X, v, train))\n    224                       for k, v in fit_params.items()])\n    225 \n    226     if parameters is not None:\n--> 227         estimator.set_params(**parameters)\n        estimator.set_params = <bound method Pipeline.set_params of Pipeline(st...one,\n            verbose=0, warm_start=False))])>\n        parameters = {'cls__max_features': 'auto', 'cls__n_estimator': 10}\n    228 \n    229     start_time = time.time()\n    230 \n    231     X_train, y_train = _safe_split(estimator, X, y, train)\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py in set_params(self=Pipeline(steps=[('cls', RandomForestClassifier(b...None,\n            verbose=0, warm_start=False))]), **kwargs={'cls__max_features': 'auto', 'cls__n_estimator': 10})\n    175 \n    176         Returns\n    177         -------\n    178         self\n    179         \"\"\"\n--> 180         self._set_params('steps', **kwargs)\n        self._set_params = <bound method Pipeline._set_params of Pipeline(s...one,\n            verbose=0, warm_start=False))])>\n        kwargs = {'cls__max_features': 'auto', 'cls__n_estimator': 10}\n    181         return self\n    182 \n    183     def _validate_steps(self):\n    184         names, estimators = zip(*self.steps)\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py in _set_params(self=Pipeline(steps=[('cls', RandomForestClassifier(b...None,\n            verbose=0, warm_start=False))]), steps_attr='steps', **params={'cls__max_features': 'auto', 'cls__n_estimator': 10})\n     64         step_names, _ = zip(*getattr(self, steps_attr))\n     65         for name in list(six.iterkeys(params)):\n     66             if '__' not in name and name in step_names:\n     67                 self._replace_step(steps_attr, name, params.pop(name))\n     68         # 3. Step parameters and other initilisation arguments\n---> 69         super(_BasePipeline, self).set_params(**params)\n        self.set_params = <bound method Pipeline.set_params of Pipeline(st...one,\n            verbose=0, warm_start=False))])>\n        params = {'cls__max_features': 'auto', 'cls__n_estimator': 10}\n     70         return self\n     71 \n     72     def _validate_names(self, names):\n     73         if len(set(names)) != len(names):\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/base.py in set_params(self=Pipeline(steps=[('cls', RandomForestClassifier(b...None,\n            verbose=0, warm_start=False))]), **params={'cls__max_features': 'auto', 'cls__n_estimator': 10})\n    279                     raise ValueError('Invalid parameter %s for estimator %s. '\n    280                                      'Check the list of available parameters '\n    281                                      'with `estimator.get_params().keys()`.' %\n    282                                      (name, self))\n    283                 sub_object = valid_params[name]\n--> 284                 sub_object.set_params(**{sub_name: value})\n        sub_object.set_params = <bound method RandomForestClassifier.set_params ...e=None,\n            verbose=0, warm_start=False)>\n        sub_name = 'n_estimator'\n        value = 10\n    285             else:\n    286                 # simple objects case\n    287                 if key not in valid_params:\n    288                     raise ValueError('Invalid parameter %s for estimator %s. '\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/base.py in set_params(self=RandomForestClassifier(bootstrap=True, class_wei...te=None,\n            verbose=0, warm_start=False), **params={'n_estimator': 10})\n    286                 # simple objects case\n    287                 if key not in valid_params:\n    288                     raise ValueError('Invalid parameter %s for estimator %s. '\n    289                                      'Check the list of available parameters '\n    290                                      'with `estimator.get_params().keys()`.' %\n--> 291                                      (key, self.__class__.__name__))\n        key = 'n_estimator'\n        self.__class__.__name__ = 'RandomForestClassifier'\n    292                 setattr(self, key, value)\n    293         return self\n    294 \n    295     def __repr__(self):\n\nValueError: Invalid parameter n_estimator for estimator RandomForestClassifier. Check the list of available parameters with `estimator.get_params().keys()`.\n___________________________________________________________________________"
     ]
    }
   ],
   "source": [
    "RF_cv = Pipeline([('cls', RandomForestClassifier()),])\n",
    "parameters = {'cls__n_estimator': (10, 20, 64, 128, 256),\n",
    "              'cls__max_features': ['auto', 'sqrt', 'log2', 'None']}\n",
    "gs_cls = GridSearchCV(RF_cv, param_grid = parameters, cv = 10, n_jobs = mp.cpu_count()-1)\n",
    "gs_cls = gs_cls.fit(X.todense(), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Best Paras:', gs_cls.best_params_)\n",
    "y_predict = gs_cls.predict(X)\n",
    "y_predict_prob = gs_cls.predict_proba(X)\n",
    "\n",
    "infile = 'predicted/RF_question_predict.csv'\n",
    "with open(infile, 'w') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter = ',', quotechar = '\"')\n",
    "    spamwriter.writerow(['question_id', 'member_id', 'question'] + cat)\n",
    "    print(len(question_test))\n",
    "    for i in range(len(question_test)):\n",
    "        writestring = [question_test[i], data_dict[question_test[i]]['member_id'], data_dict[question_test[i]]['question']]\n",
    "        writestring += list(y_predict_prob[i])\n",
    "        spamwriter.writerow(writestring)\n",
    "        \n",
    "print(metrics.classification_report(y, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         1\n",
      "          1       0.96      0.55      0.70        42\n",
      "         10       0.90      0.65      0.75       106\n",
      "         11       0.00      0.00      0.00        12\n",
      "         12       0.83      0.84      0.84       103\n",
      "          2       0.87      0.51      0.65        39\n",
      "          3       0.86      0.71      0.78        90\n",
      "          4       0.00      0.00      0.00         7\n",
      "          5       0.87      0.81      0.84        93\n",
      "          6       0.00      0.00      0.00        20\n",
      "          7       0.00      0.00      0.00        18\n",
      "          8       1.00      0.20      0.34        49\n",
      "          9       0.52      0.99      0.69       200\n",
      "\n",
      "avg / total       0.73      0.70      0.67       780\n",
      "\n",
      "780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "NB = MultinomialNB(alpha = 1.0)\n",
    "NB.fit(X_train.todense(), y_train)\n",
    "y_predict = NB.predict(X_test.todense())\n",
    "print(metrics.classification_report(y_test, y_predict))\n",
    "\n",
    "y_predict_prob = NB.predict_proba(X_test.todense())\n",
    "infile = 'predicted/NB_question_predict.csv'\n",
    "with open(infile, 'w') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter = ',', quotechar = '\"')\n",
    "    spamwriter.writerow(['question_id', 'member_id', 'question'] + cat)\n",
    "    print(len(question_test))\n",
    "    for i in range(len(question_test)):\n",
    "        writestring = [question_test[i], data_dict[question_test[i]]['member_id'], data_dict[question_test[i]]['question']]\n",
    "        writestring += list(y_predict_prob[i])\n",
    "        spamwriter.writerow(writestring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         1\n",
      "          1       1.00      0.95      0.98        42\n",
      "         10       0.96      0.93      0.95       106\n",
      "         11       1.00      0.83      0.91        12\n",
      "         12       0.95      0.97      0.96       103\n",
      "          2       0.97      0.87      0.92        39\n",
      "          3       0.90      0.88      0.89        90\n",
      "          4       1.00      0.57      0.73         7\n",
      "          5       0.87      0.99      0.92        93\n",
      "          6       1.00      0.85      0.92        20\n",
      "          7       1.00      0.61      0.76        18\n",
      "          8       0.95      0.84      0.89        49\n",
      "          9       0.87      0.95      0.91       200\n",
      "\n",
      "avg / total       0.92      0.92      0.92       780\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "RF  = RandomForestClassifier(n_jobs=-1, max_features=\"sqrt\", n_estimators=128)\n",
    "RF.fit(X_train.todense(), y_train)\n",
    "y_predicted = RF.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         1\n",
      "          1       1.00      1.00      1.00        42\n",
      "         10       0.95      0.94      0.95       106\n",
      "         11       1.00      0.67      0.80        12\n",
      "         12       0.93      0.96      0.94       103\n",
      "          2       0.95      0.92      0.94        39\n",
      "          3       0.93      0.91      0.92        90\n",
      "          4       1.00      1.00      1.00         7\n",
      "          5       0.93      0.97      0.95        93\n",
      "          6       0.90      0.95      0.93        20\n",
      "          7       0.80      0.67      0.73        18\n",
      "          8       0.95      0.86      0.90        49\n",
      "          9       0.92      0.95      0.94       200\n",
      "\n",
      "avg / total       0.93      0.93      0.93       780\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "svc = LinearSVC(C=1.0, max_iter=10000)\n",
    "svc = svc.fit(X = X_train.todense(), y = y_train)\n",
    "y_predict = svc.predict(X = X_test)\n",
    "print(metrics.classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### get fixed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape: 3875\n",
      "y train shape: (3875,)\n"
     ]
    }
   ],
   "source": [
    "X_train_text, y_train, X_test_text, y_test, member_train, member_test, question_train, question_test = dh.get_fixed_data()\n",
    "print('X train shape: {}'.format(len(X_train_text)))\n",
    "print('y train shape: {}'.format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3875, 10760)\n",
      "(1949, 10760)\n"
     ]
    }
   ],
   "source": [
    "tf = TextFeature(X_train_text, X_test_text)\n",
    "X_train, X_test = tf.get_tfidf()\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## write predicted results into file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "NB = MultinomialNB(alpha = 1.0)\n",
    "NB.fit(X_train.todense(), y_train)\n",
    "y_predict_prob = NB.predict_proba(X_test.todense())\n",
    "cat = [Grammar[item] for item in NB.classes_]\n",
    "out_NB = 'predicted/NB_predicted.csv'\n",
    "with open(out_NB, 'w') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter = ',', quotechar = '\"')\n",
    "    spamwriter.writerow(['question_id', 'member_id', 'question'] + cat)\n",
    "    for i in range(len(question_test)):\n",
    "        writestring = [question_test[i], data_dict[question_test[i]]['member_id'], data_dict[question_test[i]]['question']]\n",
    "        writestring += list(y_predict_prob[i])\n",
    "        spamwriter.writerow(writestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "RF  = RandomForestClassifier(n_jobs=-1, max_features=\"sqrt\", n_estimators=128)\n",
    "RF.fit(X_train.todense(), y_train)\n",
    "y_predict_prob = RF.predict_proba(X_test.todense())\n",
    "cat = [Grammar[item] for item in RF.classes_]\n",
    "out_RF = 'predicted/RF_predicted.csv'\n",
    "with open(out_RF, 'w') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter = ',', quotechar = '\"')\n",
    "    spamwriter.writerow(['question_id', 'member_id', 'question'] + cat)\n",
    "    for i in range(len(question_test)):\n",
    "        writestring = [question_test[i], data_dict[question_test[i]]['member_id'], data_dict[question_test[i]]['question']]\n",
    "        writestring += list(y_predict_prob[i])\n",
    "        spamwriter.writerow(writestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "SVC = LinearSVC(C=1.0, max_iter=10000)\n",
    "SVC = SVC.fit(X = X_train.todense(), y = y_train)\n",
    "y_predict = SVC.predict(X_test.todense())\n",
    "# cat = [Grammar[item] for item in SVC.classes_]\n",
    "out_SVC = 'predicted/SVC_predicted.csv'\n",
    "with open(out_SVC, 'w') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter = ',', quotechar = '\"')\n",
    "    spamwriter.writerow(['question_id', 'member_id', 'question'] + cat)\n",
    "    for i in range(len(question_test)):\n",
    "        writestring = [question_test[i], data_dict[question_test[i]]['member_id'], data_dict[question_test[i]]['question']]\n",
    "        writestring += [Grammar[y_predict[i]]]\n",
    "        spamwriter.writerow(writestring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Convert csv to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "def conver2json(infile, outfile = 'predicted/NB_question_predict.json'):\n",
    "    predict_dict = defaultdict(dict)\n",
    "\n",
    "    predict_dict = defaultdict(dict)\n",
    "    with open(infile, 'r') as csvfile:\n",
    "        for row in csv.DictReader(csvfile):\n",
    "            predict_dict[row['question_id']]['member_id'] = row['member_id']\n",
    "            predict_dict[row['question_id']]['question'] = row['question']\n",
    "            predict_dict[row['question_id']]['reply'] = 'NO!'\n",
    "            predict_dict[row['question_id']]['其它'] = row['其它']\n",
    "            predict_dict[row['question_id']]['完成式'] = row['完成式']\n",
    "            predict_dict[row['question_id']]['連接詞'] = row['連接詞']\n",
    "            predict_dict[row['question_id']]['假設語氣'] = row['假設語氣']\n",
    "            predict_dict[row['question_id']]['分詞'] = row['分詞']\n",
    "            predict_dict[row['question_id']]['進行式'] = row['進行式']\n",
    "            predict_dict[row['question_id']]['過去式'] = row['過去式']\n",
    "            predict_dict[row['question_id']]['未來式'] = row['未來式']\n",
    "            predict_dict[row['question_id']]['關係代名詞'] = row['關係代名詞']\n",
    "            predict_dict[row['question_id']]['不定詞'] = row['不定詞']\n",
    "            predict_dict[row['question_id']]['名詞子句'] = row['名詞子句']\n",
    "            predict_dict[row['question_id']]['被動'] = row['被動']\n",
    "            predict_dict[row['question_id']]['介係詞'] = row['介係詞']\n",
    "            predict_dict[row['question_id']]['question_type'] = 0\n",
    "    \n",
    "    with open(outfile, 'w') as jsonfile:\n",
    "        json.dump(predict_dict, jsonfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "infile = 'predicted/NB_question_predicted.csv'\n",
    "outfile = 'predicted/NB_question_predicted.json'\n",
    "conver2json(infile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
